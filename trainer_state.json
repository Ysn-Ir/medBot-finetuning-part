{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 16058,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015569291130174843,
      "grad_norm": 6.1862897872924805,
      "learning_rate": 2.9887920298879208e-06,
      "loss": 3.9113,
      "step": 25
    },
    {
      "epoch": 0.0031138582260349687,
      "grad_norm": 4.223369121551514,
      "learning_rate": 6.102117061021171e-06,
      "loss": 3.4246,
      "step": 50
    },
    {
      "epoch": 0.004670787339052453,
      "grad_norm": 1.4741737842559814,
      "learning_rate": 9.215442092154421e-06,
      "loss": 2.7152,
      "step": 75
    },
    {
      "epoch": 0.006227716452069937,
      "grad_norm": 1.2742819786071777,
      "learning_rate": 1.2328767123287671e-05,
      "loss": 2.4602,
      "step": 100
    },
    {
      "epoch": 0.007784645565087422,
      "grad_norm": 1.3531345129013062,
      "learning_rate": 1.5442092154420924e-05,
      "loss": 2.4034,
      "step": 125
    },
    {
      "epoch": 0.009341574678104906,
      "grad_norm": 1.4792143106460571,
      "learning_rate": 1.8555417185554173e-05,
      "loss": 2.3682,
      "step": 150
    },
    {
      "epoch": 0.01089850379112239,
      "grad_norm": 2.1116952896118164,
      "learning_rate": 2.1668742216687423e-05,
      "loss": 2.2932,
      "step": 175
    },
    {
      "epoch": 0.012455432904139875,
      "grad_norm": 1.48957359790802,
      "learning_rate": 2.4782067247820672e-05,
      "loss": 2.2533,
      "step": 200
    },
    {
      "epoch": 0.01401236201715736,
      "grad_norm": 1.5553443431854248,
      "learning_rate": 2.7895392278953925e-05,
      "loss": 2.1901,
      "step": 225
    },
    {
      "epoch": 0.015569291130174844,
      "grad_norm": 1.4846649169921875,
      "learning_rate": 3.1008717310087175e-05,
      "loss": 2.1788,
      "step": 250
    },
    {
      "epoch": 0.01712622024319233,
      "grad_norm": 1.3871304988861084,
      "learning_rate": 3.4122042341220424e-05,
      "loss": 2.1382,
      "step": 275
    },
    {
      "epoch": 0.01868314935620981,
      "grad_norm": 1.6708815097808838,
      "learning_rate": 3.7235367372353673e-05,
      "loss": 2.1929,
      "step": 300
    },
    {
      "epoch": 0.020240078469227298,
      "grad_norm": 1.4350389242172241,
      "learning_rate": 4.034869240348692e-05,
      "loss": 2.1687,
      "step": 325
    },
    {
      "epoch": 0.02179700758224478,
      "grad_norm": 1.66313636302948,
      "learning_rate": 4.346201743462018e-05,
      "loss": 2.1048,
      "step": 350
    },
    {
      "epoch": 0.023353936695262263,
      "grad_norm": 1.4732683897018433,
      "learning_rate": 4.657534246575342e-05,
      "loss": 2.1045,
      "step": 375
    },
    {
      "epoch": 0.02491086580827975,
      "grad_norm": 1.2764378786087036,
      "learning_rate": 4.968866749688668e-05,
      "loss": 2.1213,
      "step": 400
    },
    {
      "epoch": 0.026467794921297232,
      "grad_norm": 1.3286250829696655,
      "learning_rate": 5.280199252801993e-05,
      "loss": 2.0701,
      "step": 425
    },
    {
      "epoch": 0.02802472403431472,
      "grad_norm": 1.6357122659683228,
      "learning_rate": 5.591531755915318e-05,
      "loss": 2.1179,
      "step": 450
    },
    {
      "epoch": 0.0295816531473322,
      "grad_norm": 1.152483344078064,
      "learning_rate": 5.902864259028643e-05,
      "loss": 2.0878,
      "step": 475
    },
    {
      "epoch": 0.031138582260349688,
      "grad_norm": 1.4692049026489258,
      "learning_rate": 6.214196762141969e-05,
      "loss": 2.0486,
      "step": 500
    },
    {
      "epoch": 0.032695511373367174,
      "grad_norm": 1.31048583984375,
      "learning_rate": 6.525529265255293e-05,
      "loss": 2.0772,
      "step": 525
    },
    {
      "epoch": 0.03425244048638466,
      "grad_norm": 1.271628975868225,
      "learning_rate": 6.836861768368617e-05,
      "loss": 2.0744,
      "step": 550
    },
    {
      "epoch": 0.03580936959940214,
      "grad_norm": 1.2909380197525024,
      "learning_rate": 7.148194271481943e-05,
      "loss": 2.0658,
      "step": 575
    },
    {
      "epoch": 0.03736629871241962,
      "grad_norm": 1.0266145467758179,
      "learning_rate": 7.459526774595267e-05,
      "loss": 2.0881,
      "step": 600
    },
    {
      "epoch": 0.038923227825437105,
      "grad_norm": 1.162117600440979,
      "learning_rate": 7.770859277708593e-05,
      "loss": 2.081,
      "step": 625
    },
    {
      "epoch": 0.040480156938454595,
      "grad_norm": 1.2750678062438965,
      "learning_rate": 8.082191780821919e-05,
      "loss": 2.0189,
      "step": 650
    },
    {
      "epoch": 0.04203708605147208,
      "grad_norm": 1.026591181755066,
      "learning_rate": 8.393524283935243e-05,
      "loss": 2.0321,
      "step": 675
    },
    {
      "epoch": 0.04359401516448956,
      "grad_norm": 0.9574421644210815,
      "learning_rate": 8.704856787048569e-05,
      "loss": 2.0062,
      "step": 700
    },
    {
      "epoch": 0.045150944277507044,
      "grad_norm": 1.3765000104904175,
      "learning_rate": 9.016189290161894e-05,
      "loss": 1.9998,
      "step": 725
    },
    {
      "epoch": 0.046707873390524526,
      "grad_norm": 1.1486517190933228,
      "learning_rate": 9.327521793275218e-05,
      "loss": 2.021,
      "step": 750
    },
    {
      "epoch": 0.048264802503542016,
      "grad_norm": 1.013917088508606,
      "learning_rate": 9.638854296388543e-05,
      "loss": 2.0555,
      "step": 775
    },
    {
      "epoch": 0.0498217316165595,
      "grad_norm": 0.9224421977996826,
      "learning_rate": 9.950186799501868e-05,
      "loss": 1.9963,
      "step": 800
    },
    {
      "epoch": 0.05137866072957698,
      "grad_norm": 1.0198521614074707,
      "learning_rate": 0.00010261519302615194,
      "loss": 2.0054,
      "step": 825
    },
    {
      "epoch": 0.052935589842594465,
      "grad_norm": 1.1263315677642822,
      "learning_rate": 0.00010572851805728518,
      "loss": 2.0474,
      "step": 850
    },
    {
      "epoch": 0.054492518955611954,
      "grad_norm": 1.0657230615615845,
      "learning_rate": 0.00010884184308841842,
      "loss": 2.1083,
      "step": 875
    },
    {
      "epoch": 0.05604944806862944,
      "grad_norm": 1.1061066389083862,
      "learning_rate": 0.0001119551681195517,
      "loss": 2.0259,
      "step": 900
    },
    {
      "epoch": 0.05760637718164692,
      "grad_norm": 0.9444839358329773,
      "learning_rate": 0.00011506849315068494,
      "loss": 2.0602,
      "step": 925
    },
    {
      "epoch": 0.0591633062946644,
      "grad_norm": 1.0342057943344116,
      "learning_rate": 0.0001181818181818182,
      "loss": 2.0174,
      "step": 950
    },
    {
      "epoch": 0.060720235407681886,
      "grad_norm": 1.044417142868042,
      "learning_rate": 0.00012129514321295144,
      "loss": 2.0401,
      "step": 975
    },
    {
      "epoch": 0.062277164520699375,
      "grad_norm": 1.0954786539077759,
      "learning_rate": 0.0001244084682440847,
      "loss": 1.9968,
      "step": 1000
    },
    {
      "epoch": 0.06383409363371685,
      "grad_norm": 0.9978997111320496,
      "learning_rate": 0.00012752179327521794,
      "loss": 2.0473,
      "step": 1025
    },
    {
      "epoch": 0.06539102274673435,
      "grad_norm": 1.03384530544281,
      "learning_rate": 0.0001306351183063512,
      "loss": 1.9711,
      "step": 1050
    },
    {
      "epoch": 0.06694795185975183,
      "grad_norm": 1.1353569030761719,
      "learning_rate": 0.00013374844333748445,
      "loss": 2.0312,
      "step": 1075
    },
    {
      "epoch": 0.06850488097276931,
      "grad_norm": 1.115941047668457,
      "learning_rate": 0.00013686176836861768,
      "loss": 1.9762,
      "step": 1100
    },
    {
      "epoch": 0.0700618100857868,
      "grad_norm": 1.1032191514968872,
      "learning_rate": 0.00013997509339975093,
      "loss": 2.0705,
      "step": 1125
    },
    {
      "epoch": 0.07161873919880428,
      "grad_norm": 1.0177761316299438,
      "learning_rate": 0.0001430884184308842,
      "loss": 2.0097,
      "step": 1150
    },
    {
      "epoch": 0.07317566831182176,
      "grad_norm": 1.1569942235946655,
      "learning_rate": 0.00014620174346201745,
      "loss": 2.0302,
      "step": 1175
    },
    {
      "epoch": 0.07473259742483924,
      "grad_norm": 0.9441673159599304,
      "learning_rate": 0.0001493150684931507,
      "loss": 2.0415,
      "step": 1200
    },
    {
      "epoch": 0.07628952653785673,
      "grad_norm": 1.0526202917099,
      "learning_rate": 0.00015242839352428396,
      "loss": 1.9866,
      "step": 1225
    },
    {
      "epoch": 0.07784645565087421,
      "grad_norm": 1.0846617221832275,
      "learning_rate": 0.0001555417185554172,
      "loss": 1.944,
      "step": 1250
    },
    {
      "epoch": 0.0794033847638917,
      "grad_norm": 1.1078561544418335,
      "learning_rate": 0.00015865504358655044,
      "loss": 2.035,
      "step": 1275
    },
    {
      "epoch": 0.08096031387690919,
      "grad_norm": 1.4302679300308228,
      "learning_rate": 0.0001617683686176837,
      "loss": 2.0985,
      "step": 1300
    },
    {
      "epoch": 0.08251724298992667,
      "grad_norm": 1.1019222736358643,
      "learning_rate": 0.00016488169364881693,
      "loss": 1.986,
      "step": 1325
    },
    {
      "epoch": 0.08407417210294416,
      "grad_norm": 1.1954952478408813,
      "learning_rate": 0.0001679950186799502,
      "loss": 2.028,
      "step": 1350
    },
    {
      "epoch": 0.08563110121596164,
      "grad_norm": 1.1762161254882812,
      "learning_rate": 0.00017110834371108344,
      "loss": 1.9966,
      "step": 1375
    },
    {
      "epoch": 0.08718803032897912,
      "grad_norm": 1.11435067653656,
      "learning_rate": 0.0001742216687422167,
      "loss": 2.0175,
      "step": 1400
    },
    {
      "epoch": 0.0887449594419966,
      "grad_norm": 1.0930413007736206,
      "learning_rate": 0.00017733499377334995,
      "loss": 2.0449,
      "step": 1425
    },
    {
      "epoch": 0.09030188855501409,
      "grad_norm": 1.2098126411437988,
      "learning_rate": 0.0001804483188044832,
      "loss": 2.0366,
      "step": 1450
    },
    {
      "epoch": 0.09185881766803157,
      "grad_norm": 1.2020905017852783,
      "learning_rate": 0.00018356164383561644,
      "loss": 1.991,
      "step": 1475
    },
    {
      "epoch": 0.09341574678104905,
      "grad_norm": 1.2407892942428589,
      "learning_rate": 0.0001866749688667497,
      "loss": 1.9925,
      "step": 1500
    },
    {
      "epoch": 0.09497267589406655,
      "grad_norm": 1.7760124206542969,
      "learning_rate": 0.00018978829389788295,
      "loss": 2.0186,
      "step": 1525
    },
    {
      "epoch": 0.09652960500708403,
      "grad_norm": 1.187978744506836,
      "learning_rate": 0.00019290161892901618,
      "loss": 2.0045,
      "step": 1550
    },
    {
      "epoch": 0.09808653412010152,
      "grad_norm": 1.3369842767715454,
      "learning_rate": 0.00019601494396014946,
      "loss": 2.0656,
      "step": 1575
    },
    {
      "epoch": 0.099643463233119,
      "grad_norm": 1.2062076330184937,
      "learning_rate": 0.0001991282689912827,
      "loss": 1.9649,
      "step": 1600
    },
    {
      "epoch": 0.10120039234613648,
      "grad_norm": 1.4247034788131714,
      "learning_rate": 0.0001997508995294769,
      "loss": 2.0414,
      "step": 1625
    },
    {
      "epoch": 0.10275732145915396,
      "grad_norm": 1.1669936180114746,
      "learning_rate": 0.00019940492665375035,
      "loss": 2.0262,
      "step": 1650
    },
    {
      "epoch": 0.10431425057217145,
      "grad_norm": 1.3260389566421509,
      "learning_rate": 0.00019905895377802382,
      "loss": 2.0398,
      "step": 1675
    },
    {
      "epoch": 0.10587117968518893,
      "grad_norm": 1.1892428398132324,
      "learning_rate": 0.00019871298090229727,
      "loss": 2.045,
      "step": 1700
    },
    {
      "epoch": 0.10742810879820641,
      "grad_norm": 1.130735993385315,
      "learning_rate": 0.00019836700802657072,
      "loss": 2.0331,
      "step": 1725
    },
    {
      "epoch": 0.10898503791122391,
      "grad_norm": 1.2678618431091309,
      "learning_rate": 0.00019802103515084417,
      "loss": 2.0765,
      "step": 1750
    },
    {
      "epoch": 0.11054196702424139,
      "grad_norm": 1.3205933570861816,
      "learning_rate": 0.00019767506227511765,
      "loss": 2.0485,
      "step": 1775
    },
    {
      "epoch": 0.11209889613725887,
      "grad_norm": 1.0476433038711548,
      "learning_rate": 0.0001973290893993911,
      "loss": 2.0122,
      "step": 1800
    },
    {
      "epoch": 0.11365582525027636,
      "grad_norm": 1.1028486490249634,
      "learning_rate": 0.00019698311652366455,
      "loss": 1.9387,
      "step": 1825
    },
    {
      "epoch": 0.11521275436329384,
      "grad_norm": 1.0933191776275635,
      "learning_rate": 0.00019663714364793802,
      "loss": 1.9965,
      "step": 1850
    },
    {
      "epoch": 0.11676968347631132,
      "grad_norm": 1.1315280199050903,
      "learning_rate": 0.00019629117077221147,
      "loss": 2.0035,
      "step": 1875
    },
    {
      "epoch": 0.1183266125893288,
      "grad_norm": 1.240204095840454,
      "learning_rate": 0.00019594519789648492,
      "loss": 2.0488,
      "step": 1900
    },
    {
      "epoch": 0.11988354170234629,
      "grad_norm": 1.247160792350769,
      "learning_rate": 0.00019559922502075837,
      "loss": 2.0255,
      "step": 1925
    },
    {
      "epoch": 0.12144047081536377,
      "grad_norm": 1.3045260906219482,
      "learning_rate": 0.00019525325214503182,
      "loss": 1.9997,
      "step": 1950
    },
    {
      "epoch": 0.12299739992838125,
      "grad_norm": 1.2594358921051025,
      "learning_rate": 0.0001949072792693053,
      "loss": 2.0501,
      "step": 1975
    },
    {
      "epoch": 0.12455432904139875,
      "grad_norm": 1.2006717920303345,
      "learning_rate": 0.00019456130639357874,
      "loss": 2.0701,
      "step": 2000
    },
    {
      "epoch": 0.12611125815441623,
      "grad_norm": 1.3523051738739014,
      "learning_rate": 0.00019421533351785222,
      "loss": 2.045,
      "step": 2025
    },
    {
      "epoch": 0.1276681872674337,
      "grad_norm": 1.2297576665878296,
      "learning_rate": 0.00019386936064212567,
      "loss": 1.9945,
      "step": 2050
    },
    {
      "epoch": 0.1292251163804512,
      "grad_norm": 1.2127971649169922,
      "learning_rate": 0.00019352338776639915,
      "loss": 2.0048,
      "step": 2075
    },
    {
      "epoch": 0.1307820454934687,
      "grad_norm": 1.3325899839401245,
      "learning_rate": 0.00019317741489067257,
      "loss": 2.0181,
      "step": 2100
    },
    {
      "epoch": 0.13233897460648616,
      "grad_norm": 1.3606451749801636,
      "learning_rate": 0.00019283144201494602,
      "loss": 2.0289,
      "step": 2125
    },
    {
      "epoch": 0.13389590371950366,
      "grad_norm": 1.1591283082962036,
      "learning_rate": 0.0001924854691392195,
      "loss": 1.9959,
      "step": 2150
    },
    {
      "epoch": 0.13545283283252113,
      "grad_norm": 1.1622936725616455,
      "learning_rate": 0.00019213949626349294,
      "loss": 1.9202,
      "step": 2175
    },
    {
      "epoch": 0.13700976194553863,
      "grad_norm": 1.3342251777648926,
      "learning_rate": 0.00019179352338776642,
      "loss": 1.9812,
      "step": 2200
    },
    {
      "epoch": 0.1385666910585561,
      "grad_norm": 1.1631383895874023,
      "learning_rate": 0.00019144755051203987,
      "loss": 1.9698,
      "step": 2225
    },
    {
      "epoch": 0.1401236201715736,
      "grad_norm": 1.231207251548767,
      "learning_rate": 0.00019110157763631332,
      "loss": 2.0603,
      "step": 2250
    },
    {
      "epoch": 0.14168054928459106,
      "grad_norm": 1.2290997505187988,
      "learning_rate": 0.00019075560476058677,
      "loss": 1.9682,
      "step": 2275
    },
    {
      "epoch": 0.14323747839760856,
      "grad_norm": 1.4259588718414307,
      "learning_rate": 0.00019040963188486022,
      "loss": 2.0018,
      "step": 2300
    },
    {
      "epoch": 0.14479440751062603,
      "grad_norm": 1.2566417455673218,
      "learning_rate": 0.0001900636590091337,
      "loss": 2.0012,
      "step": 2325
    },
    {
      "epoch": 0.14635133662364352,
      "grad_norm": 1.162056803703308,
      "learning_rate": 0.00018971768613340714,
      "loss": 2.0455,
      "step": 2350
    },
    {
      "epoch": 0.14790826573666102,
      "grad_norm": 1.3532984256744385,
      "learning_rate": 0.00018937171325768062,
      "loss": 2.0115,
      "step": 2375
    },
    {
      "epoch": 0.1494651948496785,
      "grad_norm": 1.3255130052566528,
      "learning_rate": 0.00018902574038195407,
      "loss": 1.9907,
      "step": 2400
    },
    {
      "epoch": 0.151022123962696,
      "grad_norm": 1.2568336725234985,
      "learning_rate": 0.00018867976750622752,
      "loss": 2.0122,
      "step": 2425
    },
    {
      "epoch": 0.15257905307571346,
      "grad_norm": 1.2236328125,
      "learning_rate": 0.000188333794630501,
      "loss": 2.0233,
      "step": 2450
    },
    {
      "epoch": 0.15413598218873095,
      "grad_norm": 1.2031992673873901,
      "learning_rate": 0.00018798782175477442,
      "loss": 2.0578,
      "step": 2475
    },
    {
      "epoch": 0.15569291130174842,
      "grad_norm": 1.2527978420257568,
      "learning_rate": 0.0001876418488790479,
      "loss": 1.9666,
      "step": 2500
    },
    {
      "epoch": 0.15724984041476592,
      "grad_norm": 1.1126067638397217,
      "learning_rate": 0.00018729587600332134,
      "loss": 2.0119,
      "step": 2525
    },
    {
      "epoch": 0.1588067695277834,
      "grad_norm": 1.057523250579834,
      "learning_rate": 0.00018694990312759482,
      "loss": 1.9826,
      "step": 2550
    },
    {
      "epoch": 0.16036369864080088,
      "grad_norm": 1.4830161333084106,
      "learning_rate": 0.00018660393025186827,
      "loss": 1.983,
      "step": 2575
    },
    {
      "epoch": 0.16192062775381838,
      "grad_norm": 1.2741295099258423,
      "learning_rate": 0.00018625795737614172,
      "loss": 2.0266,
      "step": 2600
    },
    {
      "epoch": 0.16347755686683585,
      "grad_norm": 1.3754193782806396,
      "learning_rate": 0.0001859119845004152,
      "loss": 1.9826,
      "step": 2625
    },
    {
      "epoch": 0.16503448597985335,
      "grad_norm": 1.2697985172271729,
      "learning_rate": 0.00018556601162468862,
      "loss": 1.9508,
      "step": 2650
    },
    {
      "epoch": 0.16659141509287081,
      "grad_norm": 1.2549161911010742,
      "learning_rate": 0.0001852200387489621,
      "loss": 2.0087,
      "step": 2675
    },
    {
      "epoch": 0.1681483442058883,
      "grad_norm": 1.3105264902114868,
      "learning_rate": 0.00018487406587323554,
      "loss": 1.9686,
      "step": 2700
    },
    {
      "epoch": 0.16970527331890578,
      "grad_norm": 1.456024169921875,
      "learning_rate": 0.000184528092997509,
      "loss": 2.0223,
      "step": 2725
    },
    {
      "epoch": 0.17126220243192328,
      "grad_norm": 1.317236304283142,
      "learning_rate": 0.00018418212012178247,
      "loss": 2.0098,
      "step": 2750
    },
    {
      "epoch": 0.17281913154494075,
      "grad_norm": 1.0745668411254883,
      "learning_rate": 0.00018383614724605592,
      "loss": 1.9513,
      "step": 2775
    },
    {
      "epoch": 0.17437606065795824,
      "grad_norm": 1.1709109544754028,
      "learning_rate": 0.0001834901743703294,
      "loss": 2.0119,
      "step": 2800
    },
    {
      "epoch": 0.17593298977097574,
      "grad_norm": 1.2480701208114624,
      "learning_rate": 0.00018314420149460284,
      "loss": 1.9204,
      "step": 2825
    },
    {
      "epoch": 0.1774899188839932,
      "grad_norm": 1.263378381729126,
      "learning_rate": 0.0001827982286188763,
      "loss": 1.9251,
      "step": 2850
    },
    {
      "epoch": 0.1790468479970107,
      "grad_norm": 1.2970678806304932,
      "learning_rate": 0.00018245225574314974,
      "loss": 2.0247,
      "step": 2875
    },
    {
      "epoch": 0.18060377711002817,
      "grad_norm": 1.299303412437439,
      "learning_rate": 0.0001821062828674232,
      "loss": 2.0027,
      "step": 2900
    },
    {
      "epoch": 0.18216070622304567,
      "grad_norm": 1.2420884370803833,
      "learning_rate": 0.00018176030999169667,
      "loss": 1.9845,
      "step": 2925
    },
    {
      "epoch": 0.18371763533606314,
      "grad_norm": 1.2300372123718262,
      "learning_rate": 0.00018141433711597012,
      "loss": 2.0069,
      "step": 2950
    },
    {
      "epoch": 0.18527456444908064,
      "grad_norm": 1.2368751764297485,
      "learning_rate": 0.0001810683642402436,
      "loss": 2.003,
      "step": 2975
    },
    {
      "epoch": 0.1868314935620981,
      "grad_norm": 1.3032948970794678,
      "learning_rate": 0.00018072239136451704,
      "loss": 1.9584,
      "step": 3000
    },
    {
      "epoch": 0.1883884226751156,
      "grad_norm": 1.4107686281204224,
      "learning_rate": 0.00018037641848879046,
      "loss": 1.9814,
      "step": 3025
    },
    {
      "epoch": 0.1899453517881331,
      "grad_norm": 1.3066853284835815,
      "learning_rate": 0.00018003044561306394,
      "loss": 1.9507,
      "step": 3050
    },
    {
      "epoch": 0.19150228090115057,
      "grad_norm": 1.2540159225463867,
      "learning_rate": 0.0001796844727373374,
      "loss": 2.0107,
      "step": 3075
    },
    {
      "epoch": 0.19305921001416806,
      "grad_norm": 1.3850297927856445,
      "learning_rate": 0.00017933849986161087,
      "loss": 1.9765,
      "step": 3100
    },
    {
      "epoch": 0.19461613912718553,
      "grad_norm": 1.2900168895721436,
      "learning_rate": 0.00017899252698588432,
      "loss": 1.9951,
      "step": 3125
    },
    {
      "epoch": 0.19617306824020303,
      "grad_norm": 1.3963128328323364,
      "learning_rate": 0.0001786465541101578,
      "loss": 1.9766,
      "step": 3150
    },
    {
      "epoch": 0.1977299973532205,
      "grad_norm": 1.5469645261764526,
      "learning_rate": 0.00017830058123443124,
      "loss": 2.0215,
      "step": 3175
    },
    {
      "epoch": 0.199286926466238,
      "grad_norm": 1.3595118522644043,
      "learning_rate": 0.00017795460835870466,
      "loss": 2.0289,
      "step": 3200
    },
    {
      "epoch": 0.20084385557925546,
      "grad_norm": 1.2456022500991821,
      "learning_rate": 0.00017760863548297814,
      "loss": 1.9632,
      "step": 3225
    },
    {
      "epoch": 0.20240078469227296,
      "grad_norm": 1.4808220863342285,
      "learning_rate": 0.0001772626626072516,
      "loss": 1.9879,
      "step": 3250
    },
    {
      "epoch": 0.20395771380529046,
      "grad_norm": 1.340498447418213,
      "learning_rate": 0.00017691668973152507,
      "loss": 1.9804,
      "step": 3275
    },
    {
      "epoch": 0.20551464291830793,
      "grad_norm": 1.169082522392273,
      "learning_rate": 0.00017657071685579852,
      "loss": 1.9151,
      "step": 3300
    },
    {
      "epoch": 0.20707157203132542,
      "grad_norm": 1.3534945249557495,
      "learning_rate": 0.00017622474398007196,
      "loss": 1.9899,
      "step": 3325
    },
    {
      "epoch": 0.2086285011443429,
      "grad_norm": 1.2662978172302246,
      "learning_rate": 0.00017587877110434544,
      "loss": 2.0009,
      "step": 3350
    },
    {
      "epoch": 0.2101854302573604,
      "grad_norm": 1.3344396352767944,
      "learning_rate": 0.0001755327982286189,
      "loss": 1.9775,
      "step": 3375
    },
    {
      "epoch": 0.21174235937037786,
      "grad_norm": 1.1408605575561523,
      "learning_rate": 0.00017518682535289234,
      "loss": 2.02,
      "step": 3400
    },
    {
      "epoch": 0.21329928848339536,
      "grad_norm": 1.198797583580017,
      "learning_rate": 0.0001748408524771658,
      "loss": 1.9786,
      "step": 3425
    },
    {
      "epoch": 0.21485621759641282,
      "grad_norm": 1.2298667430877686,
      "learning_rate": 0.00017449487960143927,
      "loss": 1.9456,
      "step": 3450
    },
    {
      "epoch": 0.21641314670943032,
      "grad_norm": 1.2256524562835693,
      "learning_rate": 0.00017414890672571271,
      "loss": 2.0004,
      "step": 3475
    },
    {
      "epoch": 0.21797007582244782,
      "grad_norm": 1.2885044813156128,
      "learning_rate": 0.00017380293384998616,
      "loss": 2.0125,
      "step": 3500
    },
    {
      "epoch": 0.2195270049354653,
      "grad_norm": 1.3079169988632202,
      "learning_rate": 0.00017345696097425964,
      "loss": 1.9946,
      "step": 3525
    },
    {
      "epoch": 0.22108393404848278,
      "grad_norm": 1.1902518272399902,
      "learning_rate": 0.0001731109880985331,
      "loss": 2.025,
      "step": 3550
    },
    {
      "epoch": 0.22264086316150025,
      "grad_norm": 1.1041443347930908,
      "learning_rate": 0.00017276501522280654,
      "loss": 1.9678,
      "step": 3575
    },
    {
      "epoch": 0.22419779227451775,
      "grad_norm": 1.3145703077316284,
      "learning_rate": 0.00017241904234708,
      "loss": 1.9816,
      "step": 3600
    },
    {
      "epoch": 0.22575472138753522,
      "grad_norm": 1.2907437086105347,
      "learning_rate": 0.00017207306947135347,
      "loss": 1.9993,
      "step": 3625
    },
    {
      "epoch": 0.22731165050055271,
      "grad_norm": 1.230895757675171,
      "learning_rate": 0.00017172709659562691,
      "loss": 2.0199,
      "step": 3650
    },
    {
      "epoch": 0.22886857961357018,
      "grad_norm": 1.295573115348816,
      "learning_rate": 0.00017138112371990036,
      "loss": 1.9364,
      "step": 3675
    },
    {
      "epoch": 0.23042550872658768,
      "grad_norm": 1.4143767356872559,
      "learning_rate": 0.00017103515084417384,
      "loss": 2.0132,
      "step": 3700
    },
    {
      "epoch": 0.23198243783960515,
      "grad_norm": 1.2731789350509644,
      "learning_rate": 0.0001706891779684473,
      "loss": 2.0794,
      "step": 3725
    },
    {
      "epoch": 0.23353936695262265,
      "grad_norm": 1.3404104709625244,
      "learning_rate": 0.00017034320509272074,
      "loss": 2.0137,
      "step": 3750
    },
    {
      "epoch": 0.23509629606564014,
      "grad_norm": 1.150554895401001,
      "learning_rate": 0.0001699972322169942,
      "loss": 2.0027,
      "step": 3775
    },
    {
      "epoch": 0.2366532251786576,
      "grad_norm": 1.2220849990844727,
      "learning_rate": 0.00016965125934126764,
      "loss": 2.0261,
      "step": 3800
    },
    {
      "epoch": 0.2382101542916751,
      "grad_norm": 1.2344822883605957,
      "learning_rate": 0.00016930528646554111,
      "loss": 2.0537,
      "step": 3825
    },
    {
      "epoch": 0.23976708340469258,
      "grad_norm": 1.1245079040527344,
      "learning_rate": 0.00016895931358981456,
      "loss": 1.8997,
      "step": 3850
    },
    {
      "epoch": 0.24132401251771007,
      "grad_norm": 1.4450361728668213,
      "learning_rate": 0.00016861334071408804,
      "loss": 1.9754,
      "step": 3875
    },
    {
      "epoch": 0.24288094163072754,
      "grad_norm": 1.2787548303604126,
      "learning_rate": 0.0001682673678383615,
      "loss": 1.9527,
      "step": 3900
    },
    {
      "epoch": 0.24443787074374504,
      "grad_norm": 1.4747098684310913,
      "learning_rate": 0.00016792139496263494,
      "loss": 1.9354,
      "step": 3925
    },
    {
      "epoch": 0.2459947998567625,
      "grad_norm": 1.1897763013839722,
      "learning_rate": 0.0001675754220869084,
      "loss": 1.9492,
      "step": 3950
    },
    {
      "epoch": 0.24755172896978,
      "grad_norm": 1.2911468744277954,
      "learning_rate": 0.00016722944921118184,
      "loss": 1.9464,
      "step": 3975
    },
    {
      "epoch": 0.2491086580827975,
      "grad_norm": 1.265988826751709,
      "learning_rate": 0.0001668834763354553,
      "loss": 1.8862,
      "step": 4000
    },
    {
      "epoch": 0.250665587195815,
      "grad_norm": 1.1583538055419922,
      "learning_rate": 0.00016653750345972876,
      "loss": 1.9828,
      "step": 4025
    },
    {
      "epoch": 0.25222251630883247,
      "grad_norm": 1.255336880683899,
      "learning_rate": 0.00016619153058400224,
      "loss": 1.9721,
      "step": 4050
    },
    {
      "epoch": 0.25377944542184994,
      "grad_norm": 1.417746663093567,
      "learning_rate": 0.0001658455577082757,
      "loss": 2.0389,
      "step": 4075
    },
    {
      "epoch": 0.2553363745348674,
      "grad_norm": 1.3599348068237305,
      "learning_rate": 0.00016549958483254914,
      "loss": 1.9612,
      "step": 4100
    },
    {
      "epoch": 0.25689330364788493,
      "grad_norm": 1.2942312955856323,
      "learning_rate": 0.0001651536119568226,
      "loss": 1.9912,
      "step": 4125
    },
    {
      "epoch": 0.2584502327609024,
      "grad_norm": 1.3320133686065674,
      "learning_rate": 0.00016480763908109604,
      "loss": 1.9448,
      "step": 4150
    },
    {
      "epoch": 0.26000716187391987,
      "grad_norm": 1.4422465562820435,
      "learning_rate": 0.0001644616662053695,
      "loss": 1.9817,
      "step": 4175
    },
    {
      "epoch": 0.2615640909869374,
      "grad_norm": 1.303092360496521,
      "learning_rate": 0.00016411569332964296,
      "loss": 2.0009,
      "step": 4200
    },
    {
      "epoch": 0.26312102009995486,
      "grad_norm": 1.490053415298462,
      "learning_rate": 0.00016376972045391644,
      "loss": 1.9027,
      "step": 4225
    },
    {
      "epoch": 0.26467794921297233,
      "grad_norm": 1.2614487409591675,
      "learning_rate": 0.0001634237475781899,
      "loss": 2.0593,
      "step": 4250
    },
    {
      "epoch": 0.2662348783259898,
      "grad_norm": 1.252562403678894,
      "learning_rate": 0.00016307777470246334,
      "loss": 1.9908,
      "step": 4275
    },
    {
      "epoch": 0.2677918074390073,
      "grad_norm": 1.3127576112747192,
      "learning_rate": 0.00016273180182673679,
      "loss": 1.9535,
      "step": 4300
    },
    {
      "epoch": 0.2693487365520248,
      "grad_norm": 1.397889494895935,
      "learning_rate": 0.00016238582895101024,
      "loss": 1.9983,
      "step": 4325
    },
    {
      "epoch": 0.27090566566504226,
      "grad_norm": 1.284355640411377,
      "learning_rate": 0.0001620398560752837,
      "loss": 1.9788,
      "step": 4350
    },
    {
      "epoch": 0.27246259477805973,
      "grad_norm": 1.1157644987106323,
      "learning_rate": 0.00016169388319955716,
      "loss": 1.9507,
      "step": 4375
    },
    {
      "epoch": 0.27401952389107725,
      "grad_norm": 1.4464784860610962,
      "learning_rate": 0.0001613479103238306,
      "loss": 1.8601,
      "step": 4400
    },
    {
      "epoch": 0.2755764530040947,
      "grad_norm": 1.2035777568817139,
      "learning_rate": 0.0001610019374481041,
      "loss": 1.951,
      "step": 4425
    },
    {
      "epoch": 0.2771333821171122,
      "grad_norm": 1.378663420677185,
      "learning_rate": 0.00016065596457237754,
      "loss": 1.9587,
      "step": 4450
    },
    {
      "epoch": 0.2786903112301297,
      "grad_norm": 1.2642663717269897,
      "learning_rate": 0.00016030999169665099,
      "loss": 1.9101,
      "step": 4475
    },
    {
      "epoch": 0.2802472403431472,
      "grad_norm": 1.2591954469680786,
      "learning_rate": 0.00015996401882092444,
      "loss": 1.911,
      "step": 4500
    },
    {
      "epoch": 0.28180416945616465,
      "grad_norm": 1.4706180095672607,
      "learning_rate": 0.0001596180459451979,
      "loss": 1.986,
      "step": 4525
    },
    {
      "epoch": 0.2833610985691821,
      "grad_norm": 1.1587210893630981,
      "learning_rate": 0.00015927207306947136,
      "loss": 1.9656,
      "step": 4550
    },
    {
      "epoch": 0.28491802768219965,
      "grad_norm": 1.1864266395568848,
      "learning_rate": 0.0001589261001937448,
      "loss": 1.967,
      "step": 4575
    },
    {
      "epoch": 0.2864749567952171,
      "grad_norm": 1.0414029359817505,
      "learning_rate": 0.0001585801273180183,
      "loss": 1.9392,
      "step": 4600
    },
    {
      "epoch": 0.2880318859082346,
      "grad_norm": 1.1662659645080566,
      "learning_rate": 0.00015823415444229174,
      "loss": 1.9801,
      "step": 4625
    },
    {
      "epoch": 0.28958881502125206,
      "grad_norm": 1.1706843376159668,
      "learning_rate": 0.00015788818156656519,
      "loss": 1.8705,
      "step": 4650
    },
    {
      "epoch": 0.2911457441342696,
      "grad_norm": 1.457806944847107,
      "learning_rate": 0.00015754220869083863,
      "loss": 2.011,
      "step": 4675
    },
    {
      "epoch": 0.29270267324728705,
      "grad_norm": 1.324497938156128,
      "learning_rate": 0.00015719623581511208,
      "loss": 2.0242,
      "step": 4700
    },
    {
      "epoch": 0.2942596023603045,
      "grad_norm": 1.292757511138916,
      "learning_rate": 0.00015685026293938556,
      "loss": 1.9578,
      "step": 4725
    },
    {
      "epoch": 0.29581653147332204,
      "grad_norm": 1.3608779907226562,
      "learning_rate": 0.000156504290063659,
      "loss": 1.992,
      "step": 4750
    },
    {
      "epoch": 0.2973734605863395,
      "grad_norm": 1.4076263904571533,
      "learning_rate": 0.00015615831718793249,
      "loss": 1.9802,
      "step": 4775
    },
    {
      "epoch": 0.298930389699357,
      "grad_norm": 1.407801628112793,
      "learning_rate": 0.00015581234431220594,
      "loss": 1.9189,
      "step": 4800
    },
    {
      "epoch": 0.30048731881237445,
      "grad_norm": 1.4394630193710327,
      "learning_rate": 0.00015546637143647938,
      "loss": 1.9369,
      "step": 4825
    },
    {
      "epoch": 0.302044247925392,
      "grad_norm": 1.1925948858261108,
      "learning_rate": 0.00015512039856075283,
      "loss": 1.9385,
      "step": 4850
    },
    {
      "epoch": 0.30360117703840944,
      "grad_norm": 1.4182175397872925,
      "learning_rate": 0.00015477442568502628,
      "loss": 2.0634,
      "step": 4875
    },
    {
      "epoch": 0.3051581061514269,
      "grad_norm": 1.364285945892334,
      "learning_rate": 0.00015442845280929976,
      "loss": 1.9537,
      "step": 4900
    },
    {
      "epoch": 0.30671503526444444,
      "grad_norm": 1.5745460987091064,
      "learning_rate": 0.0001540824799335732,
      "loss": 1.9942,
      "step": 4925
    },
    {
      "epoch": 0.3082719643774619,
      "grad_norm": 1.3182975053787231,
      "learning_rate": 0.00015373650705784669,
      "loss": 1.9126,
      "step": 4950
    },
    {
      "epoch": 0.3098288934904794,
      "grad_norm": 1.3572183847427368,
      "learning_rate": 0.00015339053418212013,
      "loss": 1.9698,
      "step": 4975
    },
    {
      "epoch": 0.31138582260349684,
      "grad_norm": 1.2793281078338623,
      "learning_rate": 0.00015304456130639358,
      "loss": 1.945,
      "step": 5000
    },
    {
      "epoch": 0.31294275171651437,
      "grad_norm": 1.1994308233261108,
      "learning_rate": 0.00015269858843066703,
      "loss": 1.9589,
      "step": 5025
    },
    {
      "epoch": 0.31449968082953184,
      "grad_norm": 1.3007251024246216,
      "learning_rate": 0.00015235261555494048,
      "loss": 1.9269,
      "step": 5050
    },
    {
      "epoch": 0.3160566099425493,
      "grad_norm": 1.0422242879867554,
      "learning_rate": 0.00015200664267921396,
      "loss": 1.9913,
      "step": 5075
    },
    {
      "epoch": 0.3176135390555668,
      "grad_norm": 1.2543108463287354,
      "learning_rate": 0.0001516606698034874,
      "loss": 1.9676,
      "step": 5100
    },
    {
      "epoch": 0.3191704681685843,
      "grad_norm": 1.445756196975708,
      "learning_rate": 0.00015131469692776088,
      "loss": 1.9874,
      "step": 5125
    },
    {
      "epoch": 0.32072739728160177,
      "grad_norm": 1.4769517183303833,
      "learning_rate": 0.00015096872405203433,
      "loss": 1.9485,
      "step": 5150
    },
    {
      "epoch": 0.32228432639461924,
      "grad_norm": 1.1649342775344849,
      "learning_rate": 0.00015062275117630778,
      "loss": 1.9822,
      "step": 5175
    },
    {
      "epoch": 0.32384125550763676,
      "grad_norm": 1.4530998468399048,
      "learning_rate": 0.00015027677830058123,
      "loss": 1.9645,
      "step": 5200
    },
    {
      "epoch": 0.32539818462065423,
      "grad_norm": 1.1905560493469238,
      "learning_rate": 0.00014993080542485468,
      "loss": 1.8966,
      "step": 5225
    },
    {
      "epoch": 0.3269551137336717,
      "grad_norm": 1.2847020626068115,
      "learning_rate": 0.00014958483254912816,
      "loss": 1.9791,
      "step": 5250
    },
    {
      "epoch": 0.32851204284668917,
      "grad_norm": 0.9841880202293396,
      "learning_rate": 0.0001492388596734016,
      "loss": 1.9736,
      "step": 5275
    },
    {
      "epoch": 0.3300689719597067,
      "grad_norm": 1.2104647159576416,
      "learning_rate": 0.00014889288679767508,
      "loss": 1.8609,
      "step": 5300
    },
    {
      "epoch": 0.33162590107272416,
      "grad_norm": 1.5565178394317627,
      "learning_rate": 0.00014854691392194853,
      "loss": 1.9352,
      "step": 5325
    },
    {
      "epoch": 0.33318283018574163,
      "grad_norm": 1.3612526655197144,
      "learning_rate": 0.00014820094104622198,
      "loss": 1.9524,
      "step": 5350
    },
    {
      "epoch": 0.33473975929875915,
      "grad_norm": 1.24684739112854,
      "learning_rate": 0.00014785496817049543,
      "loss": 1.9669,
      "step": 5375
    },
    {
      "epoch": 0.3362966884117766,
      "grad_norm": 1.22793447971344,
      "learning_rate": 0.00014750899529476888,
      "loss": 1.9342,
      "step": 5400
    },
    {
      "epoch": 0.3378536175247941,
      "grad_norm": 1.362040400505066,
      "learning_rate": 0.00014716302241904236,
      "loss": 2.0325,
      "step": 5425
    },
    {
      "epoch": 0.33941054663781156,
      "grad_norm": 1.3810315132141113,
      "learning_rate": 0.0001468170495433158,
      "loss": 1.9306,
      "step": 5450
    },
    {
      "epoch": 0.3409674757508291,
      "grad_norm": 1.2311286926269531,
      "learning_rate": 0.00014647107666758926,
      "loss": 1.9028,
      "step": 5475
    },
    {
      "epoch": 0.34252440486384655,
      "grad_norm": 1.0710501670837402,
      "learning_rate": 0.00014612510379186273,
      "loss": 1.9374,
      "step": 5500
    },
    {
      "epoch": 0.344081333976864,
      "grad_norm": 1.3875913619995117,
      "learning_rate": 0.00014577913091613618,
      "loss": 1.9236,
      "step": 5525
    },
    {
      "epoch": 0.3456382630898815,
      "grad_norm": 1.3570935726165771,
      "learning_rate": 0.00014543315804040963,
      "loss": 1.9471,
      "step": 5550
    },
    {
      "epoch": 0.347195192202899,
      "grad_norm": 1.2234450578689575,
      "learning_rate": 0.00014508718516468308,
      "loss": 1.9553,
      "step": 5575
    },
    {
      "epoch": 0.3487521213159165,
      "grad_norm": 1.3465324640274048,
      "learning_rate": 0.00014474121228895656,
      "loss": 1.931,
      "step": 5600
    },
    {
      "epoch": 0.35030905042893395,
      "grad_norm": 1.3113133907318115,
      "learning_rate": 0.00014439523941323,
      "loss": 1.9498,
      "step": 5625
    },
    {
      "epoch": 0.3518659795419515,
      "grad_norm": 1.1646387577056885,
      "learning_rate": 0.00014404926653750346,
      "loss": 1.9238,
      "step": 5650
    },
    {
      "epoch": 0.35342290865496895,
      "grad_norm": 1.3030437231063843,
      "learning_rate": 0.00014370329366177693,
      "loss": 1.9651,
      "step": 5675
    },
    {
      "epoch": 0.3549798377679864,
      "grad_norm": 1.3991568088531494,
      "learning_rate": 0.00014335732078605038,
      "loss": 1.9727,
      "step": 5700
    },
    {
      "epoch": 0.3565367668810039,
      "grad_norm": 1.2926990985870361,
      "learning_rate": 0.00014301134791032386,
      "loss": 1.9246,
      "step": 5725
    },
    {
      "epoch": 0.3580936959940214,
      "grad_norm": 1.2621080875396729,
      "learning_rate": 0.00014266537503459728,
      "loss": 1.9431,
      "step": 5750
    },
    {
      "epoch": 0.3596506251070389,
      "grad_norm": 1.3805867433547974,
      "learning_rate": 0.00014231940215887073,
      "loss": 1.9637,
      "step": 5775
    },
    {
      "epoch": 0.36120755422005635,
      "grad_norm": 1.0297892093658447,
      "learning_rate": 0.0001419734292831442,
      "loss": 1.9196,
      "step": 5800
    },
    {
      "epoch": 0.3627644833330739,
      "grad_norm": 1.1570911407470703,
      "learning_rate": 0.00014162745640741766,
      "loss": 1.936,
      "step": 5825
    },
    {
      "epoch": 0.36432141244609134,
      "grad_norm": 1.3155598640441895,
      "learning_rate": 0.00014128148353169113,
      "loss": 1.9766,
      "step": 5850
    },
    {
      "epoch": 0.3658783415591088,
      "grad_norm": 1.3368735313415527,
      "learning_rate": 0.00014093551065596458,
      "loss": 1.9282,
      "step": 5875
    },
    {
      "epoch": 0.3674352706721263,
      "grad_norm": 1.3304930925369263,
      "learning_rate": 0.00014058953778023806,
      "loss": 1.9389,
      "step": 5900
    },
    {
      "epoch": 0.3689921997851438,
      "grad_norm": 1.3278882503509521,
      "learning_rate": 0.00014024356490451148,
      "loss": 1.9709,
      "step": 5925
    },
    {
      "epoch": 0.3705491288981613,
      "grad_norm": 1.292676568031311,
      "learning_rate": 0.00013989759202878493,
      "loss": 1.9059,
      "step": 5950
    },
    {
      "epoch": 0.37210605801117874,
      "grad_norm": 1.360101580619812,
      "learning_rate": 0.0001395516191530584,
      "loss": 1.9775,
      "step": 5975
    },
    {
      "epoch": 0.3736629871241962,
      "grad_norm": 1.3837794065475464,
      "learning_rate": 0.00013920564627733185,
      "loss": 1.9415,
      "step": 6000
    },
    {
      "epoch": 0.37521991623721374,
      "grad_norm": 1.3289573192596436,
      "learning_rate": 0.00013885967340160533,
      "loss": 1.9931,
      "step": 6025
    },
    {
      "epoch": 0.3767768453502312,
      "grad_norm": 1.3143551349639893,
      "learning_rate": 0.00013851370052587878,
      "loss": 1.9607,
      "step": 6050
    },
    {
      "epoch": 0.3783337744632487,
      "grad_norm": 1.2144988775253296,
      "learning_rate": 0.00013816772765015226,
      "loss": 1.9546,
      "step": 6075
    },
    {
      "epoch": 0.3798907035762662,
      "grad_norm": 1.195989727973938,
      "learning_rate": 0.0001378217547744257,
      "loss": 1.9056,
      "step": 6100
    },
    {
      "epoch": 0.38144763268928367,
      "grad_norm": 1.1661581993103027,
      "learning_rate": 0.00013747578189869913,
      "loss": 2.0163,
      "step": 6125
    },
    {
      "epoch": 0.38300456180230114,
      "grad_norm": 1.231531023979187,
      "learning_rate": 0.0001371298090229726,
      "loss": 1.9519,
      "step": 6150
    },
    {
      "epoch": 0.3845614909153186,
      "grad_norm": 1.343191146850586,
      "learning_rate": 0.00013678383614724605,
      "loss": 1.9711,
      "step": 6175
    },
    {
      "epoch": 0.38611842002833613,
      "grad_norm": 1.2740267515182495,
      "learning_rate": 0.00013643786327151953,
      "loss": 1.9117,
      "step": 6200
    },
    {
      "epoch": 0.3876753491413536,
      "grad_norm": 1.2457200288772583,
      "learning_rate": 0.00013609189039579298,
      "loss": 1.876,
      "step": 6225
    },
    {
      "epoch": 0.38923227825437107,
      "grad_norm": 1.1952847242355347,
      "learning_rate": 0.00013574591752006643,
      "loss": 1.9536,
      "step": 6250
    },
    {
      "epoch": 0.39078920736738854,
      "grad_norm": 1.3496654033660889,
      "learning_rate": 0.0001353999446443399,
      "loss": 1.9016,
      "step": 6275
    },
    {
      "epoch": 0.39234613648040606,
      "grad_norm": 1.2755926847457886,
      "learning_rate": 0.00013505397176861333,
      "loss": 1.923,
      "step": 6300
    },
    {
      "epoch": 0.39390306559342353,
      "grad_norm": 1.2617524862289429,
      "learning_rate": 0.0001347079988928868,
      "loss": 1.9328,
      "step": 6325
    },
    {
      "epoch": 0.395459994706441,
      "grad_norm": 1.280670404434204,
      "learning_rate": 0.00013436202601716025,
      "loss": 1.928,
      "step": 6350
    },
    {
      "epoch": 0.3970169238194585,
      "grad_norm": 1.3394709825515747,
      "learning_rate": 0.00013401605314143373,
      "loss": 1.9908,
      "step": 6375
    },
    {
      "epoch": 0.398573852932476,
      "grad_norm": 1.329905390739441,
      "learning_rate": 0.00013367008026570718,
      "loss": 1.9659,
      "step": 6400
    },
    {
      "epoch": 0.40013078204549346,
      "grad_norm": 1.1983284950256348,
      "learning_rate": 0.00013332410738998063,
      "loss": 1.9283,
      "step": 6425
    },
    {
      "epoch": 0.40168771115851093,
      "grad_norm": 1.3002865314483643,
      "learning_rate": 0.0001329781345142541,
      "loss": 1.9383,
      "step": 6450
    },
    {
      "epoch": 0.40324464027152845,
      "grad_norm": 1.336057186126709,
      "learning_rate": 0.00013263216163852755,
      "loss": 1.9207,
      "step": 6475
    },
    {
      "epoch": 0.4048015693845459,
      "grad_norm": 1.43397057056427,
      "learning_rate": 0.000132286188762801,
      "loss": 1.9256,
      "step": 6500
    },
    {
      "epoch": 0.4063584984975634,
      "grad_norm": 1.0091413259506226,
      "learning_rate": 0.00013194021588707445,
      "loss": 1.9668,
      "step": 6525
    },
    {
      "epoch": 0.4079154276105809,
      "grad_norm": 1.287381649017334,
      "learning_rate": 0.0001315942430113479,
      "loss": 1.9152,
      "step": 6550
    },
    {
      "epoch": 0.4094723567235984,
      "grad_norm": 1.2599408626556396,
      "learning_rate": 0.00013124827013562138,
      "loss": 1.9238,
      "step": 6575
    },
    {
      "epoch": 0.41102928583661585,
      "grad_norm": 1.1433804035186768,
      "learning_rate": 0.00013090229725989483,
      "loss": 1.9226,
      "step": 6600
    },
    {
      "epoch": 0.4125862149496333,
      "grad_norm": 1.114068865776062,
      "learning_rate": 0.0001305563243841683,
      "loss": 1.9002,
      "step": 6625
    },
    {
      "epoch": 0.41414314406265085,
      "grad_norm": 1.2113447189331055,
      "learning_rate": 0.00013021035150844175,
      "loss": 1.8865,
      "step": 6650
    },
    {
      "epoch": 0.4157000731756683,
      "grad_norm": 1.229640007019043,
      "learning_rate": 0.0001298643786327152,
      "loss": 1.9833,
      "step": 6675
    },
    {
      "epoch": 0.4172570022886858,
      "grad_norm": 1.2127238512039185,
      "learning_rate": 0.00012951840575698865,
      "loss": 1.9404,
      "step": 6700
    },
    {
      "epoch": 0.41881393140170325,
      "grad_norm": 1.0955162048339844,
      "learning_rate": 0.0001291724328812621,
      "loss": 1.9447,
      "step": 6725
    },
    {
      "epoch": 0.4203708605147208,
      "grad_norm": 1.4889887571334839,
      "learning_rate": 0.00012882646000553558,
      "loss": 1.9716,
      "step": 6750
    },
    {
      "epoch": 0.42192778962773825,
      "grad_norm": 1.1676753759384155,
      "learning_rate": 0.00012848048712980903,
      "loss": 1.9527,
      "step": 6775
    },
    {
      "epoch": 0.4234847187407557,
      "grad_norm": 1.3262799978256226,
      "learning_rate": 0.0001281345142540825,
      "loss": 1.9916,
      "step": 6800
    },
    {
      "epoch": 0.42504164785377324,
      "grad_norm": 1.4770108461380005,
      "learning_rate": 0.00012778854137835595,
      "loss": 1.9292,
      "step": 6825
    },
    {
      "epoch": 0.4265985769667907,
      "grad_norm": 1.3073822259902954,
      "learning_rate": 0.0001274425685026294,
      "loss": 1.9146,
      "step": 6850
    },
    {
      "epoch": 0.4281555060798082,
      "grad_norm": 1.283444881439209,
      "learning_rate": 0.00012709659562690285,
      "loss": 1.9704,
      "step": 6875
    },
    {
      "epoch": 0.42971243519282565,
      "grad_norm": 1.3431075811386108,
      "learning_rate": 0.0001267506227511763,
      "loss": 1.9222,
      "step": 6900
    },
    {
      "epoch": 0.4312693643058432,
      "grad_norm": 1.3516623973846436,
      "learning_rate": 0.00012640464987544978,
      "loss": 1.9687,
      "step": 6925
    },
    {
      "epoch": 0.43282629341886064,
      "grad_norm": 1.2797781229019165,
      "learning_rate": 0.00012605867699972323,
      "loss": 1.9418,
      "step": 6950
    },
    {
      "epoch": 0.4343832225318781,
      "grad_norm": 1.2766358852386475,
      "learning_rate": 0.0001257127041239967,
      "loss": 1.8814,
      "step": 6975
    },
    {
      "epoch": 0.43594015164489563,
      "grad_norm": 1.1517817974090576,
      "learning_rate": 0.00012536673124827015,
      "loss": 1.9051,
      "step": 7000
    },
    {
      "epoch": 0.4374970807579131,
      "grad_norm": 1.0508170127868652,
      "learning_rate": 0.0001250207583725436,
      "loss": 1.898,
      "step": 7025
    },
    {
      "epoch": 0.4390540098709306,
      "grad_norm": 1.302359938621521,
      "learning_rate": 0.00012467478549681705,
      "loss": 1.9193,
      "step": 7050
    },
    {
      "epoch": 0.44061093898394804,
      "grad_norm": 1.3109970092773438,
      "learning_rate": 0.0001243288126210905,
      "loss": 1.9509,
      "step": 7075
    },
    {
      "epoch": 0.44216786809696557,
      "grad_norm": 1.4023483991622925,
      "learning_rate": 0.00012398283974536398,
      "loss": 1.9234,
      "step": 7100
    },
    {
      "epoch": 0.44372479720998304,
      "grad_norm": 1.2509526014328003,
      "learning_rate": 0.00012363686686963743,
      "loss": 1.8851,
      "step": 7125
    },
    {
      "epoch": 0.4452817263230005,
      "grad_norm": 1.2457817792892456,
      "learning_rate": 0.0001232908939939109,
      "loss": 1.9462,
      "step": 7150
    },
    {
      "epoch": 0.446838655436018,
      "grad_norm": 1.0336368083953857,
      "learning_rate": 0.00012294492111818435,
      "loss": 1.9381,
      "step": 7175
    },
    {
      "epoch": 0.4483955845490355,
      "grad_norm": 1.1927932500839233,
      "learning_rate": 0.0001225989482424578,
      "loss": 1.9425,
      "step": 7200
    },
    {
      "epoch": 0.44995251366205297,
      "grad_norm": 1.2762997150421143,
      "learning_rate": 0.00012225297536673125,
      "loss": 1.9079,
      "step": 7225
    },
    {
      "epoch": 0.45150944277507044,
      "grad_norm": 1.1699210405349731,
      "learning_rate": 0.00012190700249100471,
      "loss": 1.9502,
      "step": 7250
    },
    {
      "epoch": 0.45306637188808796,
      "grad_norm": 1.196077823638916,
      "learning_rate": 0.00012156102961527818,
      "loss": 1.9135,
      "step": 7275
    },
    {
      "epoch": 0.45462330100110543,
      "grad_norm": 1.2461884021759033,
      "learning_rate": 0.00012121505673955163,
      "loss": 1.9545,
      "step": 7300
    },
    {
      "epoch": 0.4561802301141229,
      "grad_norm": 1.305161952972412,
      "learning_rate": 0.00012086908386382508,
      "loss": 1.9352,
      "step": 7325
    },
    {
      "epoch": 0.45773715922714037,
      "grad_norm": 1.3305261135101318,
      "learning_rate": 0.00012052311098809854,
      "loss": 1.9047,
      "step": 7350
    },
    {
      "epoch": 0.4592940883401579,
      "grad_norm": 1.492153525352478,
      "learning_rate": 0.00012017713811237199,
      "loss": 1.9018,
      "step": 7375
    },
    {
      "epoch": 0.46085101745317536,
      "grad_norm": 1.2961361408233643,
      "learning_rate": 0.00011983116523664546,
      "loss": 1.9695,
      "step": 7400
    },
    {
      "epoch": 0.46240794656619283,
      "grad_norm": 1.2809758186340332,
      "learning_rate": 0.00011948519236091891,
      "loss": 1.8856,
      "step": 7425
    },
    {
      "epoch": 0.4639648756792103,
      "grad_norm": 1.1521028280258179,
      "learning_rate": 0.00011913921948519238,
      "loss": 1.9347,
      "step": 7450
    },
    {
      "epoch": 0.4655218047922278,
      "grad_norm": 1.1717579364776611,
      "learning_rate": 0.00011879324660946583,
      "loss": 1.935,
      "step": 7475
    },
    {
      "epoch": 0.4670787339052453,
      "grad_norm": 1.2023910284042358,
      "learning_rate": 0.00011844727373373927,
      "loss": 1.8595,
      "step": 7500
    },
    {
      "epoch": 0.46863566301826276,
      "grad_norm": 1.440104603767395,
      "learning_rate": 0.00011810130085801274,
      "loss": 1.8616,
      "step": 7525
    },
    {
      "epoch": 0.4701925921312803,
      "grad_norm": 1.285789966583252,
      "learning_rate": 0.00011775532798228619,
      "loss": 1.9529,
      "step": 7550
    },
    {
      "epoch": 0.47174952124429775,
      "grad_norm": 1.2096401453018188,
      "learning_rate": 0.00011740935510655966,
      "loss": 1.9482,
      "step": 7575
    },
    {
      "epoch": 0.4733064503573152,
      "grad_norm": 1.2566499710083008,
      "learning_rate": 0.00011706338223083311,
      "loss": 1.9727,
      "step": 7600
    },
    {
      "epoch": 0.4748633794703327,
      "grad_norm": 1.2006757259368896,
      "learning_rate": 0.00011671740935510655,
      "loss": 1.9629,
      "step": 7625
    },
    {
      "epoch": 0.4764203085833502,
      "grad_norm": 1.1632429361343384,
      "learning_rate": 0.00011637143647938002,
      "loss": 1.9687,
      "step": 7650
    },
    {
      "epoch": 0.4779772376963677,
      "grad_norm": 1.3898166418075562,
      "learning_rate": 0.00011602546360365347,
      "loss": 1.8828,
      "step": 7675
    },
    {
      "epoch": 0.47953416680938515,
      "grad_norm": 1.2685108184814453,
      "learning_rate": 0.00011567949072792694,
      "loss": 1.9263,
      "step": 7700
    },
    {
      "epoch": 0.4810910959224027,
      "grad_norm": 1.077285647392273,
      "learning_rate": 0.00011533351785220039,
      "loss": 1.906,
      "step": 7725
    },
    {
      "epoch": 0.48264802503542015,
      "grad_norm": 1.1845327615737915,
      "learning_rate": 0.00011498754497647386,
      "loss": 1.8849,
      "step": 7750
    },
    {
      "epoch": 0.4842049541484376,
      "grad_norm": 1.4703199863433838,
      "learning_rate": 0.00011464157210074731,
      "loss": 1.931,
      "step": 7775
    },
    {
      "epoch": 0.4857618832614551,
      "grad_norm": 1.3253300189971924,
      "learning_rate": 0.00011429559922502076,
      "loss": 1.9103,
      "step": 7800
    },
    {
      "epoch": 0.4873188123744726,
      "grad_norm": 1.3360682725906372,
      "learning_rate": 0.00011394962634929422,
      "loss": 1.9091,
      "step": 7825
    },
    {
      "epoch": 0.4888757414874901,
      "grad_norm": 1.3168234825134277,
      "learning_rate": 0.00011360365347356767,
      "loss": 1.9177,
      "step": 7850
    },
    {
      "epoch": 0.49043267060050755,
      "grad_norm": 1.1866446733474731,
      "learning_rate": 0.00011325768059784114,
      "loss": 1.9214,
      "step": 7875
    },
    {
      "epoch": 0.491989599713525,
      "grad_norm": 1.3795584440231323,
      "learning_rate": 0.00011291170772211459,
      "loss": 1.9267,
      "step": 7900
    },
    {
      "epoch": 0.49354652882654254,
      "grad_norm": 1.3106074333190918,
      "learning_rate": 0.00011256573484638804,
      "loss": 1.9695,
      "step": 7925
    },
    {
      "epoch": 0.49510345793956,
      "grad_norm": 1.1578764915466309,
      "learning_rate": 0.00011221976197066151,
      "loss": 1.9322,
      "step": 7950
    },
    {
      "epoch": 0.4966603870525775,
      "grad_norm": 1.2758203744888306,
      "learning_rate": 0.00011187378909493496,
      "loss": 1.8675,
      "step": 7975
    },
    {
      "epoch": 0.498217316165595,
      "grad_norm": 1.3106756210327148,
      "learning_rate": 0.00011152781621920842,
      "loss": 1.9757,
      "step": 8000
    },
    {
      "epoch": 0.4997742452786125,
      "grad_norm": 1.227663278579712,
      "learning_rate": 0.00011118184334348187,
      "loss": 1.8535,
      "step": 8025
    },
    {
      "epoch": 0.50133117439163,
      "grad_norm": 1.130228877067566,
      "learning_rate": 0.00011083587046775535,
      "loss": 1.9685,
      "step": 8050
    },
    {
      "epoch": 0.5028881035046474,
      "grad_norm": 1.1936687231063843,
      "learning_rate": 0.00011048989759202879,
      "loss": 1.9255,
      "step": 8075
    },
    {
      "epoch": 0.5044450326176649,
      "grad_norm": 1.5150718688964844,
      "learning_rate": 0.00011014392471630223,
      "loss": 1.9559,
      "step": 8100
    },
    {
      "epoch": 0.5060019617306823,
      "grad_norm": 1.1177897453308105,
      "learning_rate": 0.00010979795184057571,
      "loss": 1.9057,
      "step": 8125
    },
    {
      "epoch": 0.5075588908436999,
      "grad_norm": 0.9477707743644714,
      "learning_rate": 0.00010945197896484916,
      "loss": 1.9325,
      "step": 8150
    },
    {
      "epoch": 0.5091158199567174,
      "grad_norm": 1.4040453433990479,
      "learning_rate": 0.00010910600608912262,
      "loss": 1.9523,
      "step": 8175
    },
    {
      "epoch": 0.5106727490697348,
      "grad_norm": 1.448004961013794,
      "learning_rate": 0.00010876003321339607,
      "loss": 1.8888,
      "step": 8200
    },
    {
      "epoch": 0.5122296781827523,
      "grad_norm": 1.2598763704299927,
      "learning_rate": 0.00010841406033766955,
      "loss": 1.9076,
      "step": 8225
    },
    {
      "epoch": 0.5137866072957699,
      "grad_norm": 1.593058705329895,
      "learning_rate": 0.00010806808746194298,
      "loss": 1.9292,
      "step": 8250
    },
    {
      "epoch": 0.5153435364087873,
      "grad_norm": 1.1359862089157104,
      "learning_rate": 0.00010772211458621643,
      "loss": 1.8995,
      "step": 8275
    },
    {
      "epoch": 0.5169004655218048,
      "grad_norm": 1.358139157295227,
      "learning_rate": 0.00010737614171048991,
      "loss": 1.972,
      "step": 8300
    },
    {
      "epoch": 0.5184573946348223,
      "grad_norm": 1.2194011211395264,
      "learning_rate": 0.00010703016883476336,
      "loss": 1.9204,
      "step": 8325
    },
    {
      "epoch": 0.5200143237478397,
      "grad_norm": 1.1545875072479248,
      "learning_rate": 0.00010668419595903682,
      "loss": 1.8786,
      "step": 8350
    },
    {
      "epoch": 0.5215712528608573,
      "grad_norm": 1.0465725660324097,
      "learning_rate": 0.00010633822308331027,
      "loss": 1.9154,
      "step": 8375
    },
    {
      "epoch": 0.5231281819738748,
      "grad_norm": 1.2623440027236938,
      "learning_rate": 0.00010599225020758372,
      "loss": 1.9344,
      "step": 8400
    },
    {
      "epoch": 0.5246851110868922,
      "grad_norm": 1.1514801979064941,
      "learning_rate": 0.00010564627733185718,
      "loss": 1.8968,
      "step": 8425
    },
    {
      "epoch": 0.5262420401999097,
      "grad_norm": 1.480823278427124,
      "learning_rate": 0.00010530030445613063,
      "loss": 1.9098,
      "step": 8450
    },
    {
      "epoch": 0.5277989693129271,
      "grad_norm": 1.21397864818573,
      "learning_rate": 0.00010495433158040411,
      "loss": 1.8886,
      "step": 8475
    },
    {
      "epoch": 0.5293558984259447,
      "grad_norm": 1.4445608854293823,
      "learning_rate": 0.00010460835870467756,
      "loss": 1.8871,
      "step": 8500
    },
    {
      "epoch": 0.5309128275389622,
      "grad_norm": 1.1388102769851685,
      "learning_rate": 0.00010426238582895102,
      "loss": 1.9682,
      "step": 8525
    },
    {
      "epoch": 0.5324697566519796,
      "grad_norm": 1.0156747102737427,
      "learning_rate": 0.00010391641295322447,
      "loss": 1.9285,
      "step": 8550
    },
    {
      "epoch": 0.5340266857649971,
      "grad_norm": 1.2338824272155762,
      "learning_rate": 0.00010357044007749792,
      "loss": 1.872,
      "step": 8575
    },
    {
      "epoch": 0.5355836148780146,
      "grad_norm": 1.3863399028778076,
      "learning_rate": 0.0001032244672017714,
      "loss": 1.9687,
      "step": 8600
    },
    {
      "epoch": 0.5371405439910321,
      "grad_norm": 1.1512517929077148,
      "learning_rate": 0.00010287849432604483,
      "loss": 1.8388,
      "step": 8625
    },
    {
      "epoch": 0.5386974731040496,
      "grad_norm": 1.1790988445281982,
      "learning_rate": 0.00010253252145031831,
      "loss": 1.9232,
      "step": 8650
    },
    {
      "epoch": 0.5402544022170671,
      "grad_norm": 1.1670259237289429,
      "learning_rate": 0.00010218654857459176,
      "loss": 1.8651,
      "step": 8675
    },
    {
      "epoch": 0.5418113313300845,
      "grad_norm": 1.1620280742645264,
      "learning_rate": 0.00010184057569886521,
      "loss": 1.8743,
      "step": 8700
    },
    {
      "epoch": 0.543368260443102,
      "grad_norm": 1.2116450071334839,
      "learning_rate": 0.00010149460282313867,
      "loss": 1.8972,
      "step": 8725
    },
    {
      "epoch": 0.5449251895561195,
      "grad_norm": 1.2364304065704346,
      "learning_rate": 0.00010114862994741212,
      "loss": 1.864,
      "step": 8750
    },
    {
      "epoch": 0.546482118669137,
      "grad_norm": 1.2555662393569946,
      "learning_rate": 0.0001008026570716856,
      "loss": 1.9082,
      "step": 8775
    },
    {
      "epoch": 0.5480390477821545,
      "grad_norm": 1.3062570095062256,
      "learning_rate": 0.00010045668419595903,
      "loss": 1.885,
      "step": 8800
    },
    {
      "epoch": 0.5495959768951719,
      "grad_norm": 1.3038082122802734,
      "learning_rate": 0.00010011071132023251,
      "loss": 1.9098,
      "step": 8825
    },
    {
      "epoch": 0.5511529060081894,
      "grad_norm": 1.3822097778320312,
      "learning_rate": 9.976473844450596e-05,
      "loss": 1.9729,
      "step": 8850
    },
    {
      "epoch": 0.552709835121207,
      "grad_norm": 1.1151623725891113,
      "learning_rate": 9.941876556877942e-05,
      "loss": 1.915,
      "step": 8875
    },
    {
      "epoch": 0.5542667642342244,
      "grad_norm": 1.1276068687438965,
      "learning_rate": 9.907279269305287e-05,
      "loss": 1.9282,
      "step": 8900
    },
    {
      "epoch": 0.5558236933472419,
      "grad_norm": 1.2130829095840454,
      "learning_rate": 9.872681981732632e-05,
      "loss": 1.8765,
      "step": 8925
    },
    {
      "epoch": 0.5573806224602594,
      "grad_norm": 1.2145836353302002,
      "learning_rate": 9.838084694159978e-05,
      "loss": 1.8524,
      "step": 8950
    },
    {
      "epoch": 0.5589375515732768,
      "grad_norm": 1.304821491241455,
      "learning_rate": 9.803487406587325e-05,
      "loss": 1.9623,
      "step": 8975
    },
    {
      "epoch": 0.5604944806862944,
      "grad_norm": 1.1429599523544312,
      "learning_rate": 9.76889011901467e-05,
      "loss": 1.9186,
      "step": 9000
    },
    {
      "epoch": 0.5620514097993118,
      "grad_norm": 1.2760764360427856,
      "learning_rate": 9.734292831442016e-05,
      "loss": 1.9093,
      "step": 9025
    },
    {
      "epoch": 0.5636083389123293,
      "grad_norm": 1.0164368152618408,
      "learning_rate": 9.699695543869362e-05,
      "loss": 1.9161,
      "step": 9050
    },
    {
      "epoch": 0.5651652680253468,
      "grad_norm": 1.0944275856018066,
      "learning_rate": 9.665098256296707e-05,
      "loss": 1.8493,
      "step": 9075
    },
    {
      "epoch": 0.5667221971383642,
      "grad_norm": 1.2040293216705322,
      "learning_rate": 9.630500968724052e-05,
      "loss": 1.906,
      "step": 9100
    },
    {
      "epoch": 0.5682791262513818,
      "grad_norm": 1.134379506111145,
      "learning_rate": 9.595903681151398e-05,
      "loss": 1.8724,
      "step": 9125
    },
    {
      "epoch": 0.5698360553643993,
      "grad_norm": 1.162703514099121,
      "learning_rate": 9.561306393578744e-05,
      "loss": 1.8934,
      "step": 9150
    },
    {
      "epoch": 0.5713929844774167,
      "grad_norm": 1.164411187171936,
      "learning_rate": 9.52670910600609e-05,
      "loss": 1.949,
      "step": 9175
    },
    {
      "epoch": 0.5729499135904342,
      "grad_norm": 1.332112193107605,
      "learning_rate": 9.492111818433436e-05,
      "loss": 1.8681,
      "step": 9200
    },
    {
      "epoch": 0.5745068427034518,
      "grad_norm": 1.258792519569397,
      "learning_rate": 9.45751453086078e-05,
      "loss": 1.8868,
      "step": 9225
    },
    {
      "epoch": 0.5760637718164692,
      "grad_norm": 1.358277678489685,
      "learning_rate": 9.422917243288127e-05,
      "loss": 1.9322,
      "step": 9250
    },
    {
      "epoch": 0.5776207009294867,
      "grad_norm": 1.1502264738082886,
      "learning_rate": 9.388319955715472e-05,
      "loss": 1.9485,
      "step": 9275
    },
    {
      "epoch": 0.5791776300425041,
      "grad_norm": 1.1944595575332642,
      "learning_rate": 9.353722668142818e-05,
      "loss": 1.8969,
      "step": 9300
    },
    {
      "epoch": 0.5807345591555216,
      "grad_norm": 1.2201050519943237,
      "learning_rate": 9.319125380570164e-05,
      "loss": 1.9044,
      "step": 9325
    },
    {
      "epoch": 0.5822914882685392,
      "grad_norm": 1.1389371156692505,
      "learning_rate": 9.28452809299751e-05,
      "loss": 1.9115,
      "step": 9350
    },
    {
      "epoch": 0.5838484173815566,
      "grad_norm": 1.3861461877822876,
      "learning_rate": 9.249930805424856e-05,
      "loss": 1.8232,
      "step": 9375
    },
    {
      "epoch": 0.5854053464945741,
      "grad_norm": 1.1201268434524536,
      "learning_rate": 9.2153335178522e-05,
      "loss": 1.8766,
      "step": 9400
    },
    {
      "epoch": 0.5869622756075916,
      "grad_norm": 1.1686921119689941,
      "learning_rate": 9.180736230279547e-05,
      "loss": 1.8532,
      "step": 9425
    },
    {
      "epoch": 0.588519204720609,
      "grad_norm": 1.1007922887802124,
      "learning_rate": 9.146138942706892e-05,
      "loss": 1.9385,
      "step": 9450
    },
    {
      "epoch": 0.5900761338336266,
      "grad_norm": 1.2127982378005981,
      "learning_rate": 9.111541655134238e-05,
      "loss": 1.8895,
      "step": 9475
    },
    {
      "epoch": 0.5916330629466441,
      "grad_norm": 1.0859261751174927,
      "learning_rate": 9.076944367561584e-05,
      "loss": 1.8865,
      "step": 9500
    },
    {
      "epoch": 0.5931899920596615,
      "grad_norm": 1.1290514469146729,
      "learning_rate": 9.042347079988929e-05,
      "loss": 1.9039,
      "step": 9525
    },
    {
      "epoch": 0.594746921172679,
      "grad_norm": 1.1969687938690186,
      "learning_rate": 9.007749792416274e-05,
      "loss": 1.8726,
      "step": 9550
    },
    {
      "epoch": 0.5963038502856965,
      "grad_norm": 1.2375259399414062,
      "learning_rate": 8.97315250484362e-05,
      "loss": 1.9581,
      "step": 9575
    },
    {
      "epoch": 0.597860779398714,
      "grad_norm": 1.2779933214187622,
      "learning_rate": 8.938555217270967e-05,
      "loss": 1.9096,
      "step": 9600
    },
    {
      "epoch": 0.5994177085117315,
      "grad_norm": 1.2124994993209839,
      "learning_rate": 8.903957929698312e-05,
      "loss": 1.9005,
      "step": 9625
    },
    {
      "epoch": 0.6009746376247489,
      "grad_norm": 1.3423168659210205,
      "learning_rate": 8.869360642125658e-05,
      "loss": 1.8631,
      "step": 9650
    },
    {
      "epoch": 0.6025315667377664,
      "grad_norm": 1.1073379516601562,
      "learning_rate": 8.834763354553004e-05,
      "loss": 1.9476,
      "step": 9675
    },
    {
      "epoch": 0.604088495850784,
      "grad_norm": 0.8487530946731567,
      "learning_rate": 8.800166066980349e-05,
      "loss": 1.8734,
      "step": 9700
    },
    {
      "epoch": 0.6056454249638014,
      "grad_norm": 1.313020944595337,
      "learning_rate": 8.765568779407694e-05,
      "loss": 1.8404,
      "step": 9725
    },
    {
      "epoch": 0.6072023540768189,
      "grad_norm": 1.4278315305709839,
      "learning_rate": 8.73097149183504e-05,
      "loss": 1.9248,
      "step": 9750
    },
    {
      "epoch": 0.6087592831898364,
      "grad_norm": 1.337072730064392,
      "learning_rate": 8.696374204262387e-05,
      "loss": 1.9154,
      "step": 9775
    },
    {
      "epoch": 0.6103162123028538,
      "grad_norm": 1.202937126159668,
      "learning_rate": 8.661776916689732e-05,
      "loss": 1.9181,
      "step": 9800
    },
    {
      "epoch": 0.6118731414158713,
      "grad_norm": 1.1282483339309692,
      "learning_rate": 8.627179629117078e-05,
      "loss": 1.8515,
      "step": 9825
    },
    {
      "epoch": 0.6134300705288889,
      "grad_norm": 1.2608232498168945,
      "learning_rate": 8.592582341544423e-05,
      "loss": 1.894,
      "step": 9850
    },
    {
      "epoch": 0.6149869996419063,
      "grad_norm": 1.1882139444351196,
      "learning_rate": 8.557985053971769e-05,
      "loss": 1.932,
      "step": 9875
    },
    {
      "epoch": 0.6165439287549238,
      "grad_norm": 1.128095030784607,
      "learning_rate": 8.523387766399114e-05,
      "loss": 1.8902,
      "step": 9900
    },
    {
      "epoch": 0.6181008578679412,
      "grad_norm": 1.2914716005325317,
      "learning_rate": 8.48879047882646e-05,
      "loss": 1.9397,
      "step": 9925
    },
    {
      "epoch": 0.6196577869809587,
      "grad_norm": 1.3119696378707886,
      "learning_rate": 8.454193191253807e-05,
      "loss": 1.8899,
      "step": 9950
    },
    {
      "epoch": 0.6212147160939763,
      "grad_norm": 1.1806342601776123,
      "learning_rate": 8.419595903681152e-05,
      "loss": 1.9115,
      "step": 9975
    },
    {
      "epoch": 0.6227716452069937,
      "grad_norm": 1.3449063301086426,
      "learning_rate": 8.384998616108497e-05,
      "loss": 1.8975,
      "step": 10000
    },
    {
      "epoch": 0.6243285743200112,
      "grad_norm": 1.0772777795791626,
      "learning_rate": 8.350401328535843e-05,
      "loss": 1.8901,
      "step": 10025
    },
    {
      "epoch": 0.6258855034330287,
      "grad_norm": 1.086241602897644,
      "learning_rate": 8.315804040963189e-05,
      "loss": 1.896,
      "step": 10050
    },
    {
      "epoch": 0.6274424325460461,
      "grad_norm": 1.090578317642212,
      "learning_rate": 8.281206753390534e-05,
      "loss": 1.8521,
      "step": 10075
    },
    {
      "epoch": 0.6289993616590637,
      "grad_norm": 1.1545337438583374,
      "learning_rate": 8.24660946581788e-05,
      "loss": 1.8647,
      "step": 10100
    },
    {
      "epoch": 0.6305562907720812,
      "grad_norm": 1.248337984085083,
      "learning_rate": 8.212012178245227e-05,
      "loss": 1.9164,
      "step": 10125
    },
    {
      "epoch": 0.6321132198850986,
      "grad_norm": 1.288362979888916,
      "learning_rate": 8.177414890672572e-05,
      "loss": 1.8792,
      "step": 10150
    },
    {
      "epoch": 0.6336701489981161,
      "grad_norm": 1.1908552646636963,
      "learning_rate": 8.142817603099916e-05,
      "loss": 1.8383,
      "step": 10175
    },
    {
      "epoch": 0.6352270781111335,
      "grad_norm": 1.1992534399032593,
      "learning_rate": 8.108220315527263e-05,
      "loss": 1.8873,
      "step": 10200
    },
    {
      "epoch": 0.6367840072241511,
      "grad_norm": 1.1821913719177246,
      "learning_rate": 8.073623027954609e-05,
      "loss": 1.9226,
      "step": 10225
    },
    {
      "epoch": 0.6383409363371686,
      "grad_norm": 1.267499327659607,
      "learning_rate": 8.039025740381955e-05,
      "loss": 1.9041,
      "step": 10250
    },
    {
      "epoch": 0.639897865450186,
      "grad_norm": 1.2542084455490112,
      "learning_rate": 8.0044284528093e-05,
      "loss": 1.9237,
      "step": 10275
    },
    {
      "epoch": 0.6414547945632035,
      "grad_norm": 1.0913714170455933,
      "learning_rate": 7.969831165236645e-05,
      "loss": 1.8599,
      "step": 10300
    },
    {
      "epoch": 0.6430117236762211,
      "grad_norm": 1.169389009475708,
      "learning_rate": 7.935233877663991e-05,
      "loss": 1.8701,
      "step": 10325
    },
    {
      "epoch": 0.6445686527892385,
      "grad_norm": 1.1884732246398926,
      "learning_rate": 7.900636590091336e-05,
      "loss": 1.8834,
      "step": 10350
    },
    {
      "epoch": 0.646125581902256,
      "grad_norm": 1.1679219007492065,
      "learning_rate": 7.866039302518683e-05,
      "loss": 1.9568,
      "step": 10375
    },
    {
      "epoch": 0.6476825110152735,
      "grad_norm": 1.123154640197754,
      "learning_rate": 7.831442014946029e-05,
      "loss": 1.8749,
      "step": 10400
    },
    {
      "epoch": 0.6492394401282909,
      "grad_norm": 1.2165071964263916,
      "learning_rate": 7.796844727373375e-05,
      "loss": 1.8607,
      "step": 10425
    },
    {
      "epoch": 0.6507963692413085,
      "grad_norm": 1.1780061721801758,
      "learning_rate": 7.762247439800719e-05,
      "loss": 1.8071,
      "step": 10450
    },
    {
      "epoch": 0.6523532983543259,
      "grad_norm": 1.239353895187378,
      "learning_rate": 7.727650152228065e-05,
      "loss": 1.8554,
      "step": 10475
    },
    {
      "epoch": 0.6539102274673434,
      "grad_norm": 1.2094812393188477,
      "learning_rate": 7.693052864655411e-05,
      "loss": 1.9205,
      "step": 10500
    },
    {
      "epoch": 0.6554671565803609,
      "grad_norm": 1.1807466745376587,
      "learning_rate": 7.658455577082758e-05,
      "loss": 1.9094,
      "step": 10525
    },
    {
      "epoch": 0.6570240856933783,
      "grad_norm": 1.2926149368286133,
      "learning_rate": 7.623858289510103e-05,
      "loss": 1.9144,
      "step": 10550
    },
    {
      "epoch": 0.6585810148063959,
      "grad_norm": 1.2509639263153076,
      "learning_rate": 7.589261001937449e-05,
      "loss": 1.8888,
      "step": 10575
    },
    {
      "epoch": 0.6601379439194134,
      "grad_norm": 1.1433541774749756,
      "learning_rate": 7.554663714364795e-05,
      "loss": 1.8828,
      "step": 10600
    },
    {
      "epoch": 0.6616948730324308,
      "grad_norm": 1.0918821096420288,
      "learning_rate": 7.520066426792139e-05,
      "loss": 1.9092,
      "step": 10625
    },
    {
      "epoch": 0.6632518021454483,
      "grad_norm": 1.1304727792739868,
      "learning_rate": 7.485469139219485e-05,
      "loss": 1.8832,
      "step": 10650
    },
    {
      "epoch": 0.6648087312584658,
      "grad_norm": 1.099187970161438,
      "learning_rate": 7.450871851646831e-05,
      "loss": 1.8458,
      "step": 10675
    },
    {
      "epoch": 0.6663656603714833,
      "grad_norm": 1.1642632484436035,
      "learning_rate": 7.416274564074178e-05,
      "loss": 1.8851,
      "step": 10700
    },
    {
      "epoch": 0.6679225894845008,
      "grad_norm": 1.2510406970977783,
      "learning_rate": 7.381677276501523e-05,
      "loss": 1.9012,
      "step": 10725
    },
    {
      "epoch": 0.6694795185975183,
      "grad_norm": 1.2562212944030762,
      "learning_rate": 7.347079988928869e-05,
      "loss": 1.8263,
      "step": 10750
    },
    {
      "epoch": 0.6710364477105357,
      "grad_norm": 1.0144087076187134,
      "learning_rate": 7.312482701356214e-05,
      "loss": 1.8491,
      "step": 10775
    },
    {
      "epoch": 0.6725933768235532,
      "grad_norm": 1.0114518404006958,
      "learning_rate": 7.27788541378356e-05,
      "loss": 1.8806,
      "step": 10800
    },
    {
      "epoch": 0.6741503059365707,
      "grad_norm": 1.1833343505859375,
      "learning_rate": 7.243288126210905e-05,
      "loss": 1.911,
      "step": 10825
    },
    {
      "epoch": 0.6757072350495882,
      "grad_norm": 1.1403706073760986,
      "learning_rate": 7.208690838638251e-05,
      "loss": 1.9171,
      "step": 10850
    },
    {
      "epoch": 0.6772641641626057,
      "grad_norm": 1.182288646697998,
      "learning_rate": 7.174093551065598e-05,
      "loss": 1.8648,
      "step": 10875
    },
    {
      "epoch": 0.6788210932756231,
      "grad_norm": 1.3294686079025269,
      "learning_rate": 7.139496263492943e-05,
      "loss": 1.8326,
      "step": 10900
    },
    {
      "epoch": 0.6803780223886406,
      "grad_norm": 1.2950416803359985,
      "learning_rate": 7.104898975920287e-05,
      "loss": 1.8577,
      "step": 10925
    },
    {
      "epoch": 0.6819349515016582,
      "grad_norm": 1.210033893585205,
      "learning_rate": 7.070301688347634e-05,
      "loss": 1.8993,
      "step": 10950
    },
    {
      "epoch": 0.6834918806146756,
      "grad_norm": 1.1048988103866577,
      "learning_rate": 7.03570440077498e-05,
      "loss": 1.9196,
      "step": 10975
    },
    {
      "epoch": 0.6850488097276931,
      "grad_norm": 1.1606943607330322,
      "learning_rate": 7.001107113202325e-05,
      "loss": 1.833,
      "step": 11000
    },
    {
      "epoch": 0.6866057388407106,
      "grad_norm": 1.0593042373657227,
      "learning_rate": 6.966509825629671e-05,
      "loss": 1.8404,
      "step": 11025
    },
    {
      "epoch": 0.688162667953728,
      "grad_norm": 1.2185417413711548,
      "learning_rate": 6.931912538057018e-05,
      "loss": 1.8925,
      "step": 11050
    },
    {
      "epoch": 0.6897195970667456,
      "grad_norm": 1.1965736150741577,
      "learning_rate": 6.897315250484362e-05,
      "loss": 1.9189,
      "step": 11075
    },
    {
      "epoch": 0.691276526179763,
      "grad_norm": 1.2519793510437012,
      "learning_rate": 6.862717962911707e-05,
      "loss": 1.9142,
      "step": 11100
    },
    {
      "epoch": 0.6928334552927805,
      "grad_norm": 1.1996486186981201,
      "learning_rate": 6.828120675339054e-05,
      "loss": 1.8604,
      "step": 11125
    },
    {
      "epoch": 0.694390384405798,
      "grad_norm": 1.1940034627914429,
      "learning_rate": 6.7935233877664e-05,
      "loss": 1.9297,
      "step": 11150
    },
    {
      "epoch": 0.6959473135188154,
      "grad_norm": 1.070353627204895,
      "learning_rate": 6.758926100193745e-05,
      "loss": 1.8209,
      "step": 11175
    },
    {
      "epoch": 0.697504242631833,
      "grad_norm": 1.2232414484024048,
      "learning_rate": 6.724328812621091e-05,
      "loss": 1.835,
      "step": 11200
    },
    {
      "epoch": 0.6990611717448505,
      "grad_norm": 1.122546911239624,
      "learning_rate": 6.689731525048436e-05,
      "loss": 1.8951,
      "step": 11225
    },
    {
      "epoch": 0.7006181008578679,
      "grad_norm": 1.3013112545013428,
      "learning_rate": 6.655134237475782e-05,
      "loss": 1.8813,
      "step": 11250
    },
    {
      "epoch": 0.7021750299708854,
      "grad_norm": 1.0940521955490112,
      "learning_rate": 6.620536949903127e-05,
      "loss": 1.8953,
      "step": 11275
    },
    {
      "epoch": 0.703731959083903,
      "grad_norm": 1.1304662227630615,
      "learning_rate": 6.585939662330474e-05,
      "loss": 1.8991,
      "step": 11300
    },
    {
      "epoch": 0.7052888881969204,
      "grad_norm": 1.1339247226715088,
      "learning_rate": 6.55134237475782e-05,
      "loss": 1.8309,
      "step": 11325
    },
    {
      "epoch": 0.7068458173099379,
      "grad_norm": 1.2344404458999634,
      "learning_rate": 6.516745087185165e-05,
      "loss": 1.8659,
      "step": 11350
    },
    {
      "epoch": 0.7084027464229553,
      "grad_norm": 0.995415449142456,
      "learning_rate": 6.48214779961251e-05,
      "loss": 1.8352,
      "step": 11375
    },
    {
      "epoch": 0.7099596755359728,
      "grad_norm": 1.0817455053329468,
      "learning_rate": 6.447550512039856e-05,
      "loss": 1.9211,
      "step": 11400
    },
    {
      "epoch": 0.7115166046489904,
      "grad_norm": 0.9475917220115662,
      "learning_rate": 6.412953224467202e-05,
      "loss": 1.8879,
      "step": 11425
    },
    {
      "epoch": 0.7130735337620078,
      "grad_norm": 1.1999263763427734,
      "learning_rate": 6.378355936894547e-05,
      "loss": 1.824,
      "step": 11450
    },
    {
      "epoch": 0.7146304628750253,
      "grad_norm": 0.9635406136512756,
      "learning_rate": 6.343758649321894e-05,
      "loss": 1.8085,
      "step": 11475
    },
    {
      "epoch": 0.7161873919880428,
      "grad_norm": 1.1879349946975708,
      "learning_rate": 6.30916136174924e-05,
      "loss": 1.8332,
      "step": 11500
    },
    {
      "epoch": 0.7177443211010602,
      "grad_norm": 1.2766672372817993,
      "learning_rate": 6.274564074176585e-05,
      "loss": 1.8962,
      "step": 11525
    },
    {
      "epoch": 0.7193012502140778,
      "grad_norm": 1.2812080383300781,
      "learning_rate": 6.23996678660393e-05,
      "loss": 1.9052,
      "step": 11550
    },
    {
      "epoch": 0.7208581793270953,
      "grad_norm": 1.1539942026138306,
      "learning_rate": 6.205369499031276e-05,
      "loss": 1.927,
      "step": 11575
    },
    {
      "epoch": 0.7224151084401127,
      "grad_norm": 1.1486728191375732,
      "learning_rate": 6.170772211458622e-05,
      "loss": 1.8507,
      "step": 11600
    },
    {
      "epoch": 0.7239720375531302,
      "grad_norm": 1.1352078914642334,
      "learning_rate": 6.136174923885967e-05,
      "loss": 1.8477,
      "step": 11625
    },
    {
      "epoch": 0.7255289666661477,
      "grad_norm": 1.192359447479248,
      "learning_rate": 6.1015776363133135e-05,
      "loss": 1.8637,
      "step": 11650
    },
    {
      "epoch": 0.7270858957791652,
      "grad_norm": 1.2773905992507935,
      "learning_rate": 6.06698034874066e-05,
      "loss": 1.8305,
      "step": 11675
    },
    {
      "epoch": 0.7286428248921827,
      "grad_norm": 1.2539210319519043,
      "learning_rate": 6.032383061168004e-05,
      "loss": 1.9059,
      "step": 11700
    },
    {
      "epoch": 0.7301997540052001,
      "grad_norm": 1.1078191995620728,
      "learning_rate": 5.9977857735953503e-05,
      "loss": 1.8367,
      "step": 11725
    },
    {
      "epoch": 0.7317566831182176,
      "grad_norm": 1.0932375192642212,
      "learning_rate": 5.963188486022696e-05,
      "loss": 1.8522,
      "step": 11750
    },
    {
      "epoch": 0.7333136122312351,
      "grad_norm": 1.1838635206222534,
      "learning_rate": 5.928591198450042e-05,
      "loss": 1.8493,
      "step": 11775
    },
    {
      "epoch": 0.7348705413442526,
      "grad_norm": 1.1724830865859985,
      "learning_rate": 5.893993910877388e-05,
      "loss": 1.8495,
      "step": 11800
    },
    {
      "epoch": 0.7364274704572701,
      "grad_norm": 1.1168665885925293,
      "learning_rate": 5.8593966233047335e-05,
      "loss": 1.8952,
      "step": 11825
    },
    {
      "epoch": 0.7379843995702876,
      "grad_norm": 1.2360482215881348,
      "learning_rate": 5.8247993357320784e-05,
      "loss": 1.7857,
      "step": 11850
    },
    {
      "epoch": 0.739541328683305,
      "grad_norm": 1.230881929397583,
      "learning_rate": 5.790202048159425e-05,
      "loss": 1.8382,
      "step": 11875
    },
    {
      "epoch": 0.7410982577963225,
      "grad_norm": 1.1739507913589478,
      "learning_rate": 5.75560476058677e-05,
      "loss": 1.8554,
      "step": 11900
    },
    {
      "epoch": 0.7426551869093401,
      "grad_norm": 1.1437238454818726,
      "learning_rate": 5.721007473014116e-05,
      "loss": 1.8599,
      "step": 11925
    },
    {
      "epoch": 0.7442121160223575,
      "grad_norm": 1.073040246963501,
      "learning_rate": 5.686410185441462e-05,
      "loss": 1.9036,
      "step": 11950
    },
    {
      "epoch": 0.745769045135375,
      "grad_norm": 1.1836069822311401,
      "learning_rate": 5.651812897868808e-05,
      "loss": 1.8806,
      "step": 11975
    },
    {
      "epoch": 0.7473259742483924,
      "grad_norm": 1.1581634283065796,
      "learning_rate": 5.617215610296153e-05,
      "loss": 1.8622,
      "step": 12000
    },
    {
      "epoch": 0.74888290336141,
      "grad_norm": 1.3533271551132202,
      "learning_rate": 5.582618322723498e-05,
      "loss": 1.8428,
      "step": 12025
    },
    {
      "epoch": 0.7504398324744275,
      "grad_norm": 1.087671160697937,
      "learning_rate": 5.5480210351508446e-05,
      "loss": 1.8495,
      "step": 12050
    },
    {
      "epoch": 0.7519967615874449,
      "grad_norm": 1.2619647979736328,
      "learning_rate": 5.51342374757819e-05,
      "loss": 1.8736,
      "step": 12075
    },
    {
      "epoch": 0.7535536907004624,
      "grad_norm": 1.1370115280151367,
      "learning_rate": 5.478826460005536e-05,
      "loss": 1.8886,
      "step": 12100
    },
    {
      "epoch": 0.7551106198134799,
      "grad_norm": 1.2684555053710938,
      "learning_rate": 5.444229172432882e-05,
      "loss": 1.8526,
      "step": 12125
    },
    {
      "epoch": 0.7566675489264973,
      "grad_norm": 1.16641366481781,
      "learning_rate": 5.409631884860227e-05,
      "loss": 1.8647,
      "step": 12150
    },
    {
      "epoch": 0.7582244780395149,
      "grad_norm": 1.1384085416793823,
      "learning_rate": 5.375034597287573e-05,
      "loss": 1.8913,
      "step": 12175
    },
    {
      "epoch": 0.7597814071525324,
      "grad_norm": 0.9158673882484436,
      "learning_rate": 5.340437309714918e-05,
      "loss": 1.853,
      "step": 12200
    },
    {
      "epoch": 0.7613383362655498,
      "grad_norm": 1.175403118133545,
      "learning_rate": 5.3058400221422646e-05,
      "loss": 1.8648,
      "step": 12225
    },
    {
      "epoch": 0.7628952653785673,
      "grad_norm": 1.049220323562622,
      "learning_rate": 5.27124273456961e-05,
      "loss": 1.8227,
      "step": 12250
    },
    {
      "epoch": 0.7644521944915847,
      "grad_norm": 1.132282018661499,
      "learning_rate": 5.2366454469969565e-05,
      "loss": 1.8694,
      "step": 12275
    },
    {
      "epoch": 0.7660091236046023,
      "grad_norm": 1.1060762405395508,
      "learning_rate": 5.202048159424301e-05,
      "loss": 1.8587,
      "step": 12300
    },
    {
      "epoch": 0.7675660527176198,
      "grad_norm": 1.1842718124389648,
      "learning_rate": 5.167450871851647e-05,
      "loss": 1.8898,
      "step": 12325
    },
    {
      "epoch": 0.7691229818306372,
      "grad_norm": 1.3082352876663208,
      "learning_rate": 5.1328535842789926e-05,
      "loss": 1.8832,
      "step": 12350
    },
    {
      "epoch": 0.7706799109436547,
      "grad_norm": 1.2654869556427002,
      "learning_rate": 5.098256296706338e-05,
      "loss": 1.824,
      "step": 12375
    },
    {
      "epoch": 0.7722368400566723,
      "grad_norm": 1.083756923675537,
      "learning_rate": 5.0636590091336845e-05,
      "loss": 1.9022,
      "step": 12400
    },
    {
      "epoch": 0.7737937691696897,
      "grad_norm": 1.0702370405197144,
      "learning_rate": 5.02906172156103e-05,
      "loss": 1.8866,
      "step": 12425
    },
    {
      "epoch": 0.7753506982827072,
      "grad_norm": 1.0893170833587646,
      "learning_rate": 4.994464433988376e-05,
      "loss": 1.9064,
      "step": 12450
    },
    {
      "epoch": 0.7769076273957247,
      "grad_norm": 1.115543246269226,
      "learning_rate": 4.959867146415721e-05,
      "loss": 1.7974,
      "step": 12475
    },
    {
      "epoch": 0.7784645565087421,
      "grad_norm": 1.2093905210494995,
      "learning_rate": 4.925269858843067e-05,
      "loss": 1.9085,
      "step": 12500
    },
    {
      "epoch": 0.7800214856217597,
      "grad_norm": 1.3239654302597046,
      "learning_rate": 4.8906725712704126e-05,
      "loss": 1.8716,
      "step": 12525
    },
    {
      "epoch": 0.7815784147347771,
      "grad_norm": 1.3001539707183838,
      "learning_rate": 4.856075283697759e-05,
      "loss": 1.8485,
      "step": 12550
    },
    {
      "epoch": 0.7831353438477946,
      "grad_norm": 1.1885877847671509,
      "learning_rate": 4.821477996125104e-05,
      "loss": 1.8753,
      "step": 12575
    },
    {
      "epoch": 0.7846922729608121,
      "grad_norm": 1.1731752157211304,
      "learning_rate": 4.78688070855245e-05,
      "loss": 1.8629,
      "step": 12600
    },
    {
      "epoch": 0.7862492020738295,
      "grad_norm": 1.1043611764907837,
      "learning_rate": 4.752283420979796e-05,
      "loss": 1.8339,
      "step": 12625
    },
    {
      "epoch": 0.7878061311868471,
      "grad_norm": 1.291200876235962,
      "learning_rate": 4.717686133407141e-05,
      "loss": 1.8346,
      "step": 12650
    },
    {
      "epoch": 0.7893630602998646,
      "grad_norm": 1.0460516214370728,
      "learning_rate": 4.683088845834487e-05,
      "loss": 1.8976,
      "step": 12675
    },
    {
      "epoch": 0.790919989412882,
      "grad_norm": 1.1178507804870605,
      "learning_rate": 4.6484915582618325e-05,
      "loss": 1.883,
      "step": 12700
    },
    {
      "epoch": 0.7924769185258995,
      "grad_norm": 1.165778398513794,
      "learning_rate": 4.613894270689178e-05,
      "loss": 1.8423,
      "step": 12725
    },
    {
      "epoch": 0.794033847638917,
      "grad_norm": 1.0306220054626465,
      "learning_rate": 4.579296983116524e-05,
      "loss": 1.8814,
      "step": 12750
    },
    {
      "epoch": 0.7955907767519345,
      "grad_norm": 1.029164433479309,
      "learning_rate": 4.54469969554387e-05,
      "loss": 1.8686,
      "step": 12775
    },
    {
      "epoch": 0.797147705864952,
      "grad_norm": 1.12092125415802,
      "learning_rate": 4.510102407971215e-05,
      "loss": 1.82,
      "step": 12800
    },
    {
      "epoch": 0.7987046349779695,
      "grad_norm": 1.1863734722137451,
      "learning_rate": 4.475505120398561e-05,
      "loss": 1.8581,
      "step": 12825
    },
    {
      "epoch": 0.8002615640909869,
      "grad_norm": 1.1077054738998413,
      "learning_rate": 4.440907832825907e-05,
      "loss": 1.909,
      "step": 12850
    },
    {
      "epoch": 0.8018184932040044,
      "grad_norm": 1.19893479347229,
      "learning_rate": 4.4063105452532524e-05,
      "loss": 1.9111,
      "step": 12875
    },
    {
      "epoch": 0.8033754223170219,
      "grad_norm": 1.151807188987732,
      "learning_rate": 4.371713257680598e-05,
      "loss": 1.8185,
      "step": 12900
    },
    {
      "epoch": 0.8049323514300394,
      "grad_norm": 0.9998748302459717,
      "learning_rate": 4.3371159701079437e-05,
      "loss": 1.8656,
      "step": 12925
    },
    {
      "epoch": 0.8064892805430569,
      "grad_norm": 1.1191798448562622,
      "learning_rate": 4.302518682535289e-05,
      "loss": 1.8605,
      "step": 12950
    },
    {
      "epoch": 0.8080462096560743,
      "grad_norm": 0.9492987394332886,
      "learning_rate": 4.267921394962635e-05,
      "loss": 1.8583,
      "step": 12975
    },
    {
      "epoch": 0.8096031387690918,
      "grad_norm": 1.1991565227508545,
      "learning_rate": 4.233324107389981e-05,
      "loss": 1.8831,
      "step": 13000
    },
    {
      "epoch": 0.8111600678821094,
      "grad_norm": 1.1865373849868774,
      "learning_rate": 4.198726819817326e-05,
      "loss": 1.8459,
      "step": 13025
    },
    {
      "epoch": 0.8127169969951268,
      "grad_norm": 0.9617934226989746,
      "learning_rate": 4.1641295322446724e-05,
      "loss": 1.8487,
      "step": 13050
    },
    {
      "epoch": 0.8142739261081443,
      "grad_norm": 1.1734062433242798,
      "learning_rate": 4.129532244672018e-05,
      "loss": 1.8706,
      "step": 13075
    },
    {
      "epoch": 0.8158308552211618,
      "grad_norm": 1.1996012926101685,
      "learning_rate": 4.0949349570993636e-05,
      "loss": 1.8856,
      "step": 13100
    },
    {
      "epoch": 0.8173877843341792,
      "grad_norm": 1.255298376083374,
      "learning_rate": 4.060337669526709e-05,
      "loss": 1.8358,
      "step": 13125
    },
    {
      "epoch": 0.8189447134471968,
      "grad_norm": 1.1247427463531494,
      "learning_rate": 4.025740381954055e-05,
      "loss": 1.8955,
      "step": 13150
    },
    {
      "epoch": 0.8205016425602142,
      "grad_norm": 1.194850206375122,
      "learning_rate": 3.9911430943814004e-05,
      "loss": 1.8166,
      "step": 13175
    },
    {
      "epoch": 0.8220585716732317,
      "grad_norm": 1.1607599258422852,
      "learning_rate": 3.956545806808746e-05,
      "loss": 1.8245,
      "step": 13200
    },
    {
      "epoch": 0.8236155007862492,
      "grad_norm": 1.129996418952942,
      "learning_rate": 3.921948519236092e-05,
      "loss": 1.8098,
      "step": 13225
    },
    {
      "epoch": 0.8251724298992666,
      "grad_norm": 0.9860605001449585,
      "learning_rate": 3.887351231663437e-05,
      "loss": 1.8358,
      "step": 13250
    },
    {
      "epoch": 0.8267293590122842,
      "grad_norm": 1.1753666400909424,
      "learning_rate": 3.8527539440907835e-05,
      "loss": 1.8413,
      "step": 13275
    },
    {
      "epoch": 0.8282862881253017,
      "grad_norm": 1.1194632053375244,
      "learning_rate": 3.818156656518129e-05,
      "loss": 1.8055,
      "step": 13300
    },
    {
      "epoch": 0.8298432172383191,
      "grad_norm": 1.0682446956634521,
      "learning_rate": 3.783559368945475e-05,
      "loss": 1.8018,
      "step": 13325
    },
    {
      "epoch": 0.8314001463513366,
      "grad_norm": 1.0826189517974854,
      "learning_rate": 3.7489620813728204e-05,
      "loss": 1.8433,
      "step": 13350
    },
    {
      "epoch": 0.8329570754643542,
      "grad_norm": 1.0768524408340454,
      "learning_rate": 3.7143647938001667e-05,
      "loss": 1.8568,
      "step": 13375
    },
    {
      "epoch": 0.8345140045773716,
      "grad_norm": 0.9913724660873413,
      "learning_rate": 3.679767506227512e-05,
      "loss": 1.8632,
      "step": 13400
    },
    {
      "epoch": 0.8360709336903891,
      "grad_norm": 1.0674477815628052,
      "learning_rate": 3.645170218654857e-05,
      "loss": 1.8695,
      "step": 13425
    },
    {
      "epoch": 0.8376278628034065,
      "grad_norm": 1.1690673828125,
      "learning_rate": 3.6105729310822035e-05,
      "loss": 1.8562,
      "step": 13450
    },
    {
      "epoch": 0.839184791916424,
      "grad_norm": 1.127394676208496,
      "learning_rate": 3.575975643509549e-05,
      "loss": 1.8497,
      "step": 13475
    },
    {
      "epoch": 0.8407417210294416,
      "grad_norm": 1.1611547470092773,
      "learning_rate": 3.541378355936895e-05,
      "loss": 1.8713,
      "step": 13500
    },
    {
      "epoch": 0.842298650142459,
      "grad_norm": 1.2741429805755615,
      "learning_rate": 3.50678106836424e-05,
      "loss": 1.8346,
      "step": 13525
    },
    {
      "epoch": 0.8438555792554765,
      "grad_norm": 0.9520631432533264,
      "learning_rate": 3.4721837807915866e-05,
      "loss": 1.897,
      "step": 13550
    },
    {
      "epoch": 0.845412508368494,
      "grad_norm": 1.2028695344924927,
      "learning_rate": 3.4375864932189315e-05,
      "loss": 1.8778,
      "step": 13575
    },
    {
      "epoch": 0.8469694374815114,
      "grad_norm": 1.16189706325531,
      "learning_rate": 3.402989205646278e-05,
      "loss": 1.8213,
      "step": 13600
    },
    {
      "epoch": 0.848526366594529,
      "grad_norm": 0.98760586977005,
      "learning_rate": 3.3683919180736234e-05,
      "loss": 1.8762,
      "step": 13625
    },
    {
      "epoch": 0.8500832957075465,
      "grad_norm": 1.2273001670837402,
      "learning_rate": 3.333794630500969e-05,
      "loss": 1.8412,
      "step": 13650
    },
    {
      "epoch": 0.8516402248205639,
      "grad_norm": 1.2333874702453613,
      "learning_rate": 3.2991973429283146e-05,
      "loss": 1.8484,
      "step": 13675
    },
    {
      "epoch": 0.8531971539335814,
      "grad_norm": 1.0706617832183838,
      "learning_rate": 3.26460005535566e-05,
      "loss": 1.8352,
      "step": 13700
    },
    {
      "epoch": 0.8547540830465988,
      "grad_norm": 1.215688705444336,
      "learning_rate": 3.230002767783006e-05,
      "loss": 1.8622,
      "step": 13725
    },
    {
      "epoch": 0.8563110121596164,
      "grad_norm": 1.1636511087417603,
      "learning_rate": 3.1954054802103515e-05,
      "loss": 1.8992,
      "step": 13750
    },
    {
      "epoch": 0.8578679412726339,
      "grad_norm": 1.060637354850769,
      "learning_rate": 3.160808192637698e-05,
      "loss": 1.8825,
      "step": 13775
    },
    {
      "epoch": 0.8594248703856513,
      "grad_norm": 1.0745891332626343,
      "learning_rate": 3.126210905065043e-05,
      "loss": 1.8232,
      "step": 13800
    },
    {
      "epoch": 0.8609817994986688,
      "grad_norm": 1.1209384202957153,
      "learning_rate": 3.091613617492389e-05,
      "loss": 1.7819,
      "step": 13825
    },
    {
      "epoch": 0.8625387286116863,
      "grad_norm": 1.0667308568954468,
      "learning_rate": 3.0570163299197346e-05,
      "loss": 1.8896,
      "step": 13850
    },
    {
      "epoch": 0.8640956577247038,
      "grad_norm": 1.3047304153442383,
      "learning_rate": 3.02241904234708e-05,
      "loss": 1.7729,
      "step": 13875
    },
    {
      "epoch": 0.8656525868377213,
      "grad_norm": 1.2019705772399902,
      "learning_rate": 2.9878217547744258e-05,
      "loss": 1.8778,
      "step": 13900
    },
    {
      "epoch": 0.8672095159507388,
      "grad_norm": 1.0889509916305542,
      "learning_rate": 2.9532244672017718e-05,
      "loss": 1.8007,
      "step": 13925
    },
    {
      "epoch": 0.8687664450637562,
      "grad_norm": 1.1785331964492798,
      "learning_rate": 2.918627179629117e-05,
      "loss": 1.7999,
      "step": 13950
    },
    {
      "epoch": 0.8703233741767737,
      "grad_norm": 1.1928602457046509,
      "learning_rate": 2.884029892056463e-05,
      "loss": 1.8396,
      "step": 13975
    },
    {
      "epoch": 0.8718803032897913,
      "grad_norm": 0.96885746717453,
      "learning_rate": 2.849432604483809e-05,
      "loss": 1.8258,
      "step": 14000
    },
    {
      "epoch": 0.8734372324028087,
      "grad_norm": 1.1113340854644775,
      "learning_rate": 2.8148353169111542e-05,
      "loss": 1.8443,
      "step": 14025
    },
    {
      "epoch": 0.8749941615158262,
      "grad_norm": 1.0854947566986084,
      "learning_rate": 2.7802380293384998e-05,
      "loss": 1.8434,
      "step": 14050
    },
    {
      "epoch": 0.8765510906288436,
      "grad_norm": 1.2535401582717896,
      "learning_rate": 2.7456407417658458e-05,
      "loss": 1.9106,
      "step": 14075
    },
    {
      "epoch": 0.8781080197418611,
      "grad_norm": 1.2845476865768433,
      "learning_rate": 2.711043454193191e-05,
      "loss": 1.8341,
      "step": 14100
    },
    {
      "epoch": 0.8796649488548787,
      "grad_norm": 1.0563325881958008,
      "learning_rate": 2.676446166620537e-05,
      "loss": 1.8184,
      "step": 14125
    },
    {
      "epoch": 0.8812218779678961,
      "grad_norm": 1.1709779500961304,
      "learning_rate": 2.641848879047883e-05,
      "loss": 1.839,
      "step": 14150
    },
    {
      "epoch": 0.8827788070809136,
      "grad_norm": 1.1542282104492188,
      "learning_rate": 2.6072515914752282e-05,
      "loss": 1.8256,
      "step": 14175
    },
    {
      "epoch": 0.8843357361939311,
      "grad_norm": 1.1019405126571655,
      "learning_rate": 2.572654303902574e-05,
      "loss": 1.8179,
      "step": 14200
    },
    {
      "epoch": 0.8858926653069485,
      "grad_norm": 0.9942153692245483,
      "learning_rate": 2.53805701632992e-05,
      "loss": 1.8268,
      "step": 14225
    },
    {
      "epoch": 0.8874495944199661,
      "grad_norm": 1.2157474756240845,
      "learning_rate": 2.5034597287572654e-05,
      "loss": 1.8723,
      "step": 14250
    },
    {
      "epoch": 0.8890065235329836,
      "grad_norm": 1.1639492511749268,
      "learning_rate": 2.4688624411846113e-05,
      "loss": 1.8575,
      "step": 14275
    },
    {
      "epoch": 0.890563452646001,
      "grad_norm": 1.3289638757705688,
      "learning_rate": 2.434265153611957e-05,
      "loss": 1.8654,
      "step": 14300
    },
    {
      "epoch": 0.8921203817590185,
      "grad_norm": 1.172029972076416,
      "learning_rate": 2.3996678660393025e-05,
      "loss": 1.8725,
      "step": 14325
    },
    {
      "epoch": 0.893677310872036,
      "grad_norm": 1.225519061088562,
      "learning_rate": 2.365070578466648e-05,
      "loss": 1.8221,
      "step": 14350
    },
    {
      "epoch": 0.8952342399850535,
      "grad_norm": 1.1006488800048828,
      "learning_rate": 2.330473290893994e-05,
      "loss": 1.849,
      "step": 14375
    },
    {
      "epoch": 0.896791169098071,
      "grad_norm": 1.0821585655212402,
      "learning_rate": 2.2958760033213397e-05,
      "loss": 1.8672,
      "step": 14400
    },
    {
      "epoch": 0.8983480982110884,
      "grad_norm": 1.1886974573135376,
      "learning_rate": 2.2612787157486853e-05,
      "loss": 1.8913,
      "step": 14425
    },
    {
      "epoch": 0.8999050273241059,
      "grad_norm": 1.0395348072052002,
      "learning_rate": 2.2266814281760312e-05,
      "loss": 1.809,
      "step": 14450
    },
    {
      "epoch": 0.9014619564371235,
      "grad_norm": 1.2798347473144531,
      "learning_rate": 2.192084140603377e-05,
      "loss": 1.8434,
      "step": 14475
    },
    {
      "epoch": 0.9030188855501409,
      "grad_norm": 1.1775566339492798,
      "learning_rate": 2.1574868530307225e-05,
      "loss": 1.8567,
      "step": 14500
    },
    {
      "epoch": 0.9045758146631584,
      "grad_norm": 1.0684494972229004,
      "learning_rate": 2.1228895654580684e-05,
      "loss": 1.8704,
      "step": 14525
    },
    {
      "epoch": 0.9061327437761759,
      "grad_norm": 1.095188856124878,
      "learning_rate": 2.088292277885414e-05,
      "loss": 1.8357,
      "step": 14550
    },
    {
      "epoch": 0.9076896728891933,
      "grad_norm": 1.252223253250122,
      "learning_rate": 2.0536949903127596e-05,
      "loss": 1.7654,
      "step": 14575
    },
    {
      "epoch": 0.9092466020022109,
      "grad_norm": 1.043718934059143,
      "learning_rate": 2.0190977027401052e-05,
      "loss": 1.8219,
      "step": 14600
    },
    {
      "epoch": 0.9108035311152283,
      "grad_norm": 1.132658839225769,
      "learning_rate": 1.984500415167451e-05,
      "loss": 1.8788,
      "step": 14625
    },
    {
      "epoch": 0.9123604602282458,
      "grad_norm": 1.2053680419921875,
      "learning_rate": 1.9499031275947965e-05,
      "loss": 1.8772,
      "step": 14650
    },
    {
      "epoch": 0.9139173893412633,
      "grad_norm": 1.234432339668274,
      "learning_rate": 1.9153058400221424e-05,
      "loss": 1.8666,
      "step": 14675
    },
    {
      "epoch": 0.9154743184542807,
      "grad_norm": 1.0480461120605469,
      "learning_rate": 1.880708552449488e-05,
      "loss": 1.818,
      "step": 14700
    },
    {
      "epoch": 0.9170312475672983,
      "grad_norm": 1.1130744218826294,
      "learning_rate": 1.8461112648768336e-05,
      "loss": 1.7941,
      "step": 14725
    },
    {
      "epoch": 0.9185881766803158,
      "grad_norm": 1.0793299674987793,
      "learning_rate": 1.8115139773041796e-05,
      "loss": 1.828,
      "step": 14750
    },
    {
      "epoch": 0.9201451057933332,
      "grad_norm": 1.2389918565750122,
      "learning_rate": 1.7769166897315252e-05,
      "loss": 1.8318,
      "step": 14775
    },
    {
      "epoch": 0.9217020349063507,
      "grad_norm": 1.1321711540222168,
      "learning_rate": 1.7423194021588708e-05,
      "loss": 1.8332,
      "step": 14800
    },
    {
      "epoch": 0.9232589640193682,
      "grad_norm": 1.077357292175293,
      "learning_rate": 1.7077221145862164e-05,
      "loss": 1.824,
      "step": 14825
    },
    {
      "epoch": 0.9248158931323857,
      "grad_norm": 1.1337957382202148,
      "learning_rate": 1.673124827013562e-05,
      "loss": 1.8447,
      "step": 14850
    },
    {
      "epoch": 0.9263728222454032,
      "grad_norm": 1.2307343482971191,
      "learning_rate": 1.638527539440908e-05,
      "loss": 1.8761,
      "step": 14875
    },
    {
      "epoch": 0.9279297513584206,
      "grad_norm": 1.0277830362319946,
      "learning_rate": 1.6039302518682536e-05,
      "loss": 1.8404,
      "step": 14900
    },
    {
      "epoch": 0.9294866804714381,
      "grad_norm": 1.0866074562072754,
      "learning_rate": 1.5693329642955992e-05,
      "loss": 1.8495,
      "step": 14925
    },
    {
      "epoch": 0.9310436095844556,
      "grad_norm": 1.183817982673645,
      "learning_rate": 1.534735676722945e-05,
      "loss": 1.8516,
      "step": 14950
    },
    {
      "epoch": 0.9326005386974731,
      "grad_norm": 1.0707697868347168,
      "learning_rate": 1.5001383891502907e-05,
      "loss": 1.8606,
      "step": 14975
    },
    {
      "epoch": 0.9341574678104906,
      "grad_norm": 1.057004451751709,
      "learning_rate": 1.4655411015776363e-05,
      "loss": 1.8335,
      "step": 15000
    },
    {
      "epoch": 0.9357143969235081,
      "grad_norm": 0.9287494421005249,
      "learning_rate": 1.4309438140049821e-05,
      "loss": 1.8591,
      "step": 15025
    },
    {
      "epoch": 0.9372713260365255,
      "grad_norm": 1.0978593826293945,
      "learning_rate": 1.3963465264323277e-05,
      "loss": 1.8231,
      "step": 15050
    },
    {
      "epoch": 0.938828255149543,
      "grad_norm": 1.09346342086792,
      "learning_rate": 1.3617492388596733e-05,
      "loss": 1.8297,
      "step": 15075
    },
    {
      "epoch": 0.9403851842625606,
      "grad_norm": 1.0443350076675415,
      "learning_rate": 1.3271519512870193e-05,
      "loss": 1.8483,
      "step": 15100
    },
    {
      "epoch": 0.941942113375578,
      "grad_norm": 1.210855484008789,
      "learning_rate": 1.2925546637143649e-05,
      "loss": 1.8711,
      "step": 15125
    },
    {
      "epoch": 0.9434990424885955,
      "grad_norm": 1.0742167234420776,
      "learning_rate": 1.2579573761417105e-05,
      "loss": 1.7873,
      "step": 15150
    },
    {
      "epoch": 0.945055971601613,
      "grad_norm": 1.1063097715377808,
      "learning_rate": 1.2233600885690561e-05,
      "loss": 1.8196,
      "step": 15175
    },
    {
      "epoch": 0.9466129007146304,
      "grad_norm": 1.1391289234161377,
      "learning_rate": 1.1887628009964019e-05,
      "loss": 1.8825,
      "step": 15200
    },
    {
      "epoch": 0.948169829827648,
      "grad_norm": 1.036202311515808,
      "learning_rate": 1.1541655134237477e-05,
      "loss": 1.8201,
      "step": 15225
    },
    {
      "epoch": 0.9497267589406654,
      "grad_norm": 1.1772716045379639,
      "learning_rate": 1.1195682258510935e-05,
      "loss": 1.8506,
      "step": 15250
    },
    {
      "epoch": 0.9512836880536829,
      "grad_norm": 0.9255304336547852,
      "learning_rate": 1.084970938278439e-05,
      "loss": 1.8218,
      "step": 15275
    },
    {
      "epoch": 0.9528406171667004,
      "grad_norm": 1.1946910619735718,
      "learning_rate": 1.0503736507057847e-05,
      "loss": 1.8301,
      "step": 15300
    },
    {
      "epoch": 0.9543975462797178,
      "grad_norm": 1.183937907218933,
      "learning_rate": 1.0157763631331305e-05,
      "loss": 1.8522,
      "step": 15325
    },
    {
      "epoch": 0.9559544753927354,
      "grad_norm": 1.056274652481079,
      "learning_rate": 9.81179075560476e-06,
      "loss": 1.836,
      "step": 15350
    },
    {
      "epoch": 0.9575114045057529,
      "grad_norm": 1.1381193399429321,
      "learning_rate": 9.465817879878218e-06,
      "loss": 1.8707,
      "step": 15375
    },
    {
      "epoch": 0.9590683336187703,
      "grad_norm": 0.962604284286499,
      "learning_rate": 9.119845004151674e-06,
      "loss": 1.8609,
      "step": 15400
    },
    {
      "epoch": 0.9606252627317878,
      "grad_norm": 0.9988597631454468,
      "learning_rate": 8.77387212842513e-06,
      "loss": 1.8406,
      "step": 15425
    },
    {
      "epoch": 0.9621821918448054,
      "grad_norm": 1.0466357469558716,
      "learning_rate": 8.427899252698588e-06,
      "loss": 1.8452,
      "step": 15450
    },
    {
      "epoch": 0.9637391209578228,
      "grad_norm": 1.1986627578735352,
      "learning_rate": 8.081926376972046e-06,
      "loss": 1.8244,
      "step": 15475
    },
    {
      "epoch": 0.9652960500708403,
      "grad_norm": 1.090736746788025,
      "learning_rate": 7.735953501245504e-06,
      "loss": 1.8356,
      "step": 15500
    },
    {
      "epoch": 0.9668529791838577,
      "grad_norm": 1.1533302068710327,
      "learning_rate": 7.389980625518959e-06,
      "loss": 1.832,
      "step": 15525
    },
    {
      "epoch": 0.9684099082968752,
      "grad_norm": 1.0735447406768799,
      "learning_rate": 7.044007749792417e-06,
      "loss": 1.7863,
      "step": 15550
    },
    {
      "epoch": 0.9699668374098928,
      "grad_norm": 1.213388204574585,
      "learning_rate": 6.698034874065874e-06,
      "loss": 1.8428,
      "step": 15575
    },
    {
      "epoch": 0.9715237665229102,
      "grad_norm": 1.1173725128173828,
      "learning_rate": 6.35206199833933e-06,
      "loss": 1.7821,
      "step": 15600
    },
    {
      "epoch": 0.9730806956359277,
      "grad_norm": 1.138095498085022,
      "learning_rate": 6.006089122612788e-06,
      "loss": 1.786,
      "step": 15625
    },
    {
      "epoch": 0.9746376247489452,
      "grad_norm": 1.238510251045227,
      "learning_rate": 5.660116246886244e-06,
      "loss": 1.83,
      "step": 15650
    },
    {
      "epoch": 0.9761945538619626,
      "grad_norm": 1.233586072921753,
      "learning_rate": 5.314143371159702e-06,
      "loss": 1.8042,
      "step": 15675
    },
    {
      "epoch": 0.9777514829749802,
      "grad_norm": 1.168821930885315,
      "learning_rate": 4.968170495433158e-06,
      "loss": 1.8809,
      "step": 15700
    },
    {
      "epoch": 0.9793084120879977,
      "grad_norm": 1.0607730150222778,
      "learning_rate": 4.6221976197066156e-06,
      "loss": 1.8419,
      "step": 15725
    },
    {
      "epoch": 0.9808653412010151,
      "grad_norm": 0.954241156578064,
      "learning_rate": 4.2762247439800725e-06,
      "loss": 1.7923,
      "step": 15750
    },
    {
      "epoch": 0.9824222703140326,
      "grad_norm": 1.0968780517578125,
      "learning_rate": 3.930251868253529e-06,
      "loss": 1.8304,
      "step": 15775
    },
    {
      "epoch": 0.98397919942705,
      "grad_norm": 1.1509150266647339,
      "learning_rate": 3.5842789925269864e-06,
      "loss": 1.8287,
      "step": 15800
    },
    {
      "epoch": 0.9855361285400676,
      "grad_norm": 1.23102605342865,
      "learning_rate": 3.238306116800443e-06,
      "loss": 1.7541,
      "step": 15825
    },
    {
      "epoch": 0.9870930576530851,
      "grad_norm": 1.0962074995040894,
      "learning_rate": 2.8923332410739e-06,
      "loss": 1.831,
      "step": 15850
    },
    {
      "epoch": 0.9886499867661025,
      "grad_norm": 1.1522223949432373,
      "learning_rate": 2.5463603653473568e-06,
      "loss": 1.8238,
      "step": 15875
    },
    {
      "epoch": 0.99020691587912,
      "grad_norm": 1.1718626022338867,
      "learning_rate": 2.200387489620814e-06,
      "loss": 1.8641,
      "step": 15900
    },
    {
      "epoch": 0.9917638449921375,
      "grad_norm": 1.251891016960144,
      "learning_rate": 1.8544146138942706e-06,
      "loss": 1.8562,
      "step": 15925
    },
    {
      "epoch": 0.993320774105155,
      "grad_norm": 1.1162112951278687,
      "learning_rate": 1.5084417381677276e-06,
      "loss": 1.7818,
      "step": 15950
    },
    {
      "epoch": 0.9948777032181725,
      "grad_norm": 1.1467863321304321,
      "learning_rate": 1.1624688624411847e-06,
      "loss": 1.8764,
      "step": 15975
    },
    {
      "epoch": 0.99643463233119,
      "grad_norm": 1.2182166576385498,
      "learning_rate": 8.164959867146417e-07,
      "loss": 1.8094,
      "step": 16000
    },
    {
      "epoch": 0.9979915614442074,
      "grad_norm": 0.9950308799743652,
      "learning_rate": 4.7052311098809857e-07,
      "loss": 1.8209,
      "step": 16025
    },
    {
      "epoch": 0.999548490557225,
      "grad_norm": 1.2304974794387817,
      "learning_rate": 1.245502352615555e-07,
      "loss": 1.8868,
      "step": 16050
    }
  ],
  "logging_steps": 25,
  "max_steps": 16058,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.1014577066183885e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
